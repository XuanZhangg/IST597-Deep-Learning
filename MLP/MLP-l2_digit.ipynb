{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: (50000, 784)\n",
      "val_size: (10000, 784)\n",
      "test_size: (10000, 784)\n",
      "train_output_size (50000,)\n",
      "val_output_size: (10000,)\n",
      "test_output_size (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load and normalize data\n",
    "(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = tf.reshape(X_train, (X_train.shape[0],-1))/255\n",
    "X_test = tf.reshape(X_test, (X_test.shape[0],-1))/255\n",
    "\n",
    "#reserve the last 10000 training examples for validation\n",
    "X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "print(\"train_size:\", X_train.shape)\n",
    "print(\"val_size:\", X_val.shape)\n",
    "print(\"test_size:\", X_test.shape)\n",
    "print(\"train_output_size\", y_train.shape)\n",
    "print(\"val_output_size:\", y_val.shape)\n",
    "print(\"test_output_size\", y_test.shape)\n",
    "\n",
    "size_input = X_train.shape[1]\n",
    "size_output = len(set(y_train))\n",
    "size_hidden1 = 256\n",
    "size_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        regularizer: str or None\n",
    "        R_lambda: the parameter for regularizer\n",
    "        drop_prob: 0 to 1\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_output, device\n",
    "        \n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "        \n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
    "        # Initialize biases for hidden layer 1\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights between hidden layer 1 and hidden layer 2\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
    "        # Initialize biases for hidden layer 2\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "\n",
    "         # Initialize weights between hidden layer 2 and output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "        # Initialize the state of Adam-yogi algorithm\n",
    "        self.v_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.v_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.s_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.s_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        \n",
    "        self.v_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.v_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.s_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.s_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        \n",
    "        self.v_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.v_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.s_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.s_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        \n",
    "        self.v_state = [self.v_W1,self.v_W2,self.v_W3,self.v_b1,self.v_b2,self.v_b3]\n",
    "        self.s_state = [self.s_W1,self.s_W2,self.s_W3,self.s_b1,self.s_b2,self.s_b3]\n",
    "         \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        def compute_output(X):\n",
    "            # Cast X to float32\n",
    "            X_tf = tf.cast(X, dtype=tf.float32)\n",
    "            \n",
    "            #set the dropout prob\n",
    "            prob = self.drop_prob\n",
    "\n",
    "            # Remember to normalize your dataset before moving forward\n",
    "            # Compute values in hidden layer 1\n",
    "            what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "            hhat1 = tf.nn.experimental.stateless_dropout(tf.nn.sigmoid(what1), rate = prob, seed = [1,0])\n",
    "\n",
    "            # Compute values in hidden layer 2\n",
    "            what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "            hhat2 = tf.nn.experimental.stateless_dropout(tf.nn.sigmoid(what2), rate = prob, seed = [1,0])\n",
    "\n",
    "            # Compute output\n",
    "            output = tf.nn.softmax(tf.matmul(hhat2, self.W3) + self.b3)\n",
    "\n",
    "            return output\n",
    "        \n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = compute_output(X)\n",
    "        else:\n",
    "            self.y = compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true,y_pred, from_logits = False)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "        \n",
    "    def backward(self, X_train, y_train, hyperparams):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "\n",
    "        #use adam optimizer\n",
    "        '''\n",
    "        adam optimizer\n",
    "        '''\n",
    "        beta1, beta2, eps = 0.9, 0.999, 1e-6\n",
    "\n",
    "        for p, v, s, grad in zip(self.variables, self.v_state, self.s_state, grads):\n",
    "            v[:].assign(beta1 * v  + (1 - beta1) * grad)\n",
    "            s[:].assign(s + (1 - beta2) * tf.math.sign(tf.math.square(grad) - s) * tf.math.square(grad))\n",
    "            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
    "            p[:].assign(p - hyperparams['lr'] * v_bias_corr/ (tf.math.sqrt(s_bias_corr) + eps))\n",
    "            \n",
    "\n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))\n",
    "    \n",
    "#     def dropout_layer(self,X, dropout):\n",
    "#         assert 0 <= dropout <= 1\n",
    "#         # In this case, all elements are dropped out\n",
    "#         if dropout == 1:\n",
    "#             return tf.zeros_like(X)\n",
    "#         # In this case, all elements are kept\n",
    "#         if dropout == 0:\n",
    "#             return X\n",
    "#         mask = tf.random.uniform(\n",
    "#             shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\n",
    "#         return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.8145 - Val loss: 0.7957 - Test loss: 0.8138 - Train acc:= 75.90% - Val acc:= 76.76% - Test acc:= 75.84%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.5413 - Val loss: 0.5595 - Test loss: 0.5693 - Train acc:= 83.85% - Val acc:= 83.86% - Test acc:= 83.46%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.4237 - Val loss: 0.4636 - Test loss: 0.4737 - Train acc:= 87.31% - Val acc:= 86.67% - Test acc:= 86.33%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.3518 - Val loss: 0.4081 - Test loss: 0.4154 - Train acc:= 89.44% - Val acc:= 88.37% - Test acc:= 87.88%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.3009 - Val loss: 0.3697 - Test loss: 0.3750 - Train acc:= 90.93% - Val acc:= 89.47% - Test acc:= 88.83%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.2620 - Val loss: 0.3401 - Test loss: 0.3447 - Train acc:= 92.15% - Val acc:= 90.37% - Test acc:= 89.93%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.2311 - Val loss: 0.3165 - Test loss: 0.3209 - Train acc:= 93.08% - Val acc:= 91.12% - Test acc:= 90.69%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.2056 - Val loss: 0.2972 - Test loss: 0.3016 - Train acc:= 93.92% - Val acc:= 91.53% - Test acc:= 91.19%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.1844 - Val loss: 0.2807 - Test loss: 0.2855 - Train acc:= 94.60% - Val acc:= 92.09% - Test acc:= 91.72%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.1663 - Val loss: 0.2666 - Test loss: 0.2718 - Train acc:= 95.16% - Val acc:= 92.54% - Test acc:= 92.11%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.1508 - Val loss: 0.2541 - Test loss: 0.2598 - Train acc:= 95.71% - Val acc:= 92.89% - Test acc:= 92.52%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.1373 - Val loss: 0.2430 - Test loss: 0.2491 - Train acc:= 96.18% - Val acc:= 93.14% - Test acc:= 92.86%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.1255 - Val loss: 0.2328 - Test loss: 0.2394 - Train acc:= 96.55% - Val acc:= 93.50% - Test acc:= 93.14%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.1151 - Val loss: 0.2235 - Test loss: 0.2306 - Train acc:= 96.93% - Val acc:= 93.77% - Test acc:= 93.37%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.1058 - Val loss: 0.2149 - Test loss: 0.2224 - Train acc:= 97.21% - Val acc:= 94.02% - Test acc:= 93.62%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.0975 - Val loss: 0.2068 - Test loss: 0.2147 - Train acc:= 97.51% - Val acc:= 94.32% - Test acc:= 93.91%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.0900 - Val loss: 0.1991 - Test loss: 0.2074 - Train acc:= 97.71% - Val acc:= 94.53% - Test acc:= 94.07%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.0832 - Val loss: 0.1918 - Test loss: 0.2005 - Train acc:= 97.95% - Val acc:= 94.75% - Test acc:= 94.17%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.0769 - Val loss: 0.1848 - Test loss: 0.1938 - Train acc:= 98.15% - Val acc:= 94.95% - Test acc:= 94.34%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.0713 - Val loss: 0.1782 - Test loss: 0.1873 - Train acc:= 98.34% - Val acc:= 95.14% - Test acc:= 94.53%\n",
      "\n",
      "Total time taken (in seconds): 1264.88\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 0.8380 - Val loss: 0.8497 - Test loss: 0.8425 - Train acc:= 75.16% - Val acc:= 75.45% - Test acc:= 74.68%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 0.5587 - Val loss: 0.5968 - Test loss: 0.5882 - Train acc:= 83.11% - Val acc:= 82.44% - Test acc:= 82.43%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 0.4361 - Val loss: 0.4923 - Test loss: 0.4818 - Train acc:= 86.90% - Val acc:= 85.95% - Test acc:= 85.42%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 0.3619 - Val loss: 0.4317 - Test loss: 0.4206 - Train acc:= 89.12% - Val acc:= 87.71% - Test acc:= 87.48%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 0.3097 - Val loss: 0.3908 - Test loss: 0.3793 - Train acc:= 90.71% - Val acc:= 88.79% - Test acc:= 88.75%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 0.2702 - Val loss: 0.3604 - Test loss: 0.3483 - Train acc:= 91.97% - Val acc:= 89.77% - Test acc:= 89.76%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 0.2388 - Val loss: 0.3365 - Test loss: 0.3235 - Train acc:= 92.99% - Val acc:= 90.36% - Test acc:= 90.38%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 0.2131 - Val loss: 0.3164 - Test loss: 0.3029 - Train acc:= 93.81% - Val acc:= 90.93% - Test acc:= 91.03%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 0.1914 - Val loss: 0.2990 - Test loss: 0.2855 - Train acc:= 94.47% - Val acc:= 91.41% - Test acc:= 91.50%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 0.1729 - Val loss: 0.2835 - Test loss: 0.2704 - Train acc:= 95.05% - Val acc:= 91.84% - Test acc:= 92.00%\n",
      "Number of Simulation = 2 - Number of Epoch = 110\n",
      "Train loss:= 0.1570 - Val loss: 0.2697 - Test loss: 0.2571 - Train acc:= 95.52% - Val acc:= 92.21% - Test acc:= 92.38%\n",
      "Number of Simulation = 2 - Number of Epoch = 120\n",
      "Train loss:= 0.1430 - Val loss: 0.2572 - Test loss: 0.2452 - Train acc:= 95.94% - Val acc:= 92.59% - Test acc:= 92.79%\n",
      "Number of Simulation = 2 - Number of Epoch = 130\n",
      "Train loss:= 0.1306 - Val loss: 0.2458 - Test loss: 0.2344 - Train acc:= 96.34% - Val acc:= 92.92% - Test acc:= 93.11%\n",
      "Number of Simulation = 2 - Number of Epoch = 140\n",
      "Train loss:= 0.1196 - Val loss: 0.2355 - Test loss: 0.2247 - Train acc:= 96.68% - Val acc:= 93.22% - Test acc:= 93.41%\n",
      "Number of Simulation = 2 - Number of Epoch = 150\n",
      "Train loss:= 0.1098 - Val loss: 0.2260 - Test loss: 0.2158 - Train acc:= 96.99% - Val acc:= 93.43% - Test acc:= 93.66%\n",
      "Number of Simulation = 2 - Number of Epoch = 160\n",
      "Train loss:= 0.1010 - Val loss: 0.2174 - Test loss: 0.2077 - Train acc:= 97.28% - Val acc:= 93.68% - Test acc:= 93.83%\n",
      "Number of Simulation = 2 - Number of Epoch = 170\n",
      "Train loss:= 0.0932 - Val loss: 0.2094 - Test loss: 0.2001 - Train acc:= 97.55% - Val acc:= 93.93% - Test acc:= 94.14%\n",
      "Number of Simulation = 2 - Number of Epoch = 180\n",
      "Train loss:= 0.0862 - Val loss: 0.2020 - Test loss: 0.1931 - Train acc:= 97.78% - Val acc:= 94.19% - Test acc:= 94.30%\n",
      "Number of Simulation = 2 - Number of Epoch = 190\n",
      "Train loss:= 0.0799 - Val loss: 0.1951 - Test loss: 0.1865 - Train acc:= 98.00% - Val acc:= 94.33% - Test acc:= 94.48%\n",
      "Number of Simulation = 2 - Number of Epoch = 200\n",
      "Train loss:= 0.0742 - Val loss: 0.1886 - Test loss: 0.1803 - Train acc:= 98.18% - Val acc:= 94.58% - Test acc:= 94.68%\n",
      "\n",
      "Total time taken (in seconds): 1269.14\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 0.7927 - Val loss: 0.7842 - Test loss: 0.7940 - Train acc:= 76.16% - Val acc:= 76.58% - Test acc:= 76.23%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 0.5319 - Val loss: 0.5457 - Test loss: 0.5549 - Train acc:= 83.92% - Val acc:= 83.71% - Test acc:= 83.22%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 0.4171 - Val loss: 0.4484 - Test loss: 0.4571 - Train acc:= 87.40% - Val acc:= 86.49% - Test acc:= 86.15%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 0.3478 - Val loss: 0.3926 - Test loss: 0.4000 - Train acc:= 89.54% - Val acc:= 88.05% - Test acc:= 87.94%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 0.2988 - Val loss: 0.3539 - Test loss: 0.3613 - Train acc:= 91.03% - Val acc:= 89.26% - Test acc:= 89.21%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 0.2614 - Val loss: 0.3247 - Test loss: 0.3327 - Train acc:= 92.21% - Val acc:= 90.20% - Test acc:= 90.24%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 0.2316 - Val loss: 0.3016 - Test loss: 0.3100 - Train acc:= 93.10% - Val acc:= 91.00% - Test acc:= 90.77%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 0.2071 - Val loss: 0.2825 - Test loss: 0.2916 - Train acc:= 93.76% - Val acc:= 91.63% - Test acc:= 91.15%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 0.1866 - Val loss: 0.2662 - Test loss: 0.2763 - Train acc:= 94.44% - Val acc:= 92.06% - Test acc:= 91.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 0.1690 - Val loss: 0.2522 - Test loss: 0.2630 - Train acc:= 95.04% - Val acc:= 92.38% - Test acc:= 92.12%\n",
      "Number of Simulation = 3 - Number of Epoch = 110\n",
      "Train loss:= 0.1538 - Val loss: 0.2399 - Test loss: 0.2513 - Train acc:= 95.54% - Val acc:= 92.77% - Test acc:= 92.44%\n",
      "Number of Simulation = 3 - Number of Epoch = 120\n",
      "Train loss:= 0.1405 - Val loss: 0.2291 - Test loss: 0.2406 - Train acc:= 95.97% - Val acc:= 93.06% - Test acc:= 92.69%\n",
      "Number of Simulation = 3 - Number of Epoch = 130\n",
      "Train loss:= 0.1288 - Val loss: 0.2194 - Test loss: 0.2308 - Train acc:= 96.36% - Val acc:= 93.39% - Test acc:= 93.00%\n",
      "Number of Simulation = 3 - Number of Epoch = 140\n",
      "Train loss:= 0.1183 - Val loss: 0.2107 - Test loss: 0.2217 - Train acc:= 96.72% - Val acc:= 93.72% - Test acc:= 93.37%\n",
      "Number of Simulation = 3 - Number of Epoch = 150\n",
      "Train loss:= 0.1089 - Val loss: 0.2027 - Test loss: 0.2131 - Train acc:= 97.07% - Val acc:= 94.05% - Test acc:= 93.59%\n",
      "Number of Simulation = 3 - Number of Epoch = 160\n",
      "Train loss:= 0.1005 - Val loss: 0.1953 - Test loss: 0.2051 - Train acc:= 97.37% - Val acc:= 94.25% - Test acc:= 93.79%\n",
      "Number of Simulation = 3 - Number of Epoch = 170\n",
      "Train loss:= 0.0929 - Val loss: 0.1883 - Test loss: 0.1977 - Train acc:= 97.62% - Val acc:= 94.36% - Test acc:= 94.08%\n",
      "Number of Simulation = 3 - Number of Epoch = 180\n",
      "Train loss:= 0.0860 - Val loss: 0.1817 - Test loss: 0.1907 - Train acc:= 97.83% - Val acc:= 94.64% - Test acc:= 94.29%\n",
      "Number of Simulation = 3 - Number of Epoch = 190\n",
      "Train loss:= 0.0797 - Val loss: 0.1755 - Test loss: 0.1841 - Train acc:= 98.04% - Val acc:= 94.74% - Test acc:= 94.47%\n",
      "Number of Simulation = 3 - Number of Epoch = 200\n",
      "Train loss:= 0.0740 - Val loss: 0.1695 - Test loss: 0.1778 - Train acc:= 98.23% - Val acc:= 94.84% - Test acc:= 94.67%\n",
      "\n",
      "Total time taken (in seconds): 1279.19\n",
      "Number of Simulation = 4 - Number of Epoch = 10\n",
      "Train loss:= 0.8186 - Val loss: 0.7801 - Test loss: 0.8070 - Train acc:= 74.63% - Val acc:= 75.31% - Test acc:= 75.26%\n",
      "Number of Simulation = 4 - Number of Epoch = 20\n",
      "Train loss:= 0.5469 - Val loss: 0.5397 - Test loss: 0.5710 - Train acc:= 83.26% - Val acc:= 83.44% - Test acc:= 82.30%\n",
      "Number of Simulation = 4 - Number of Epoch = 30\n",
      "Train loss:= 0.4313 - Val loss: 0.4484 - Test loss: 0.4766 - Train acc:= 86.93% - Val acc:= 86.20% - Test acc:= 85.31%\n",
      "Number of Simulation = 4 - Number of Epoch = 40\n",
      "Train loss:= 0.3607 - Val loss: 0.3961 - Test loss: 0.4193 - Train acc:= 89.14% - Val acc:= 87.87% - Test acc:= 87.34%\n",
      "Number of Simulation = 4 - Number of Epoch = 50\n",
      "Train loss:= 0.3107 - Val loss: 0.3596 - Test loss: 0.3784 - Train acc:= 90.66% - Val acc:= 89.01% - Test acc:= 88.53%\n",
      "Number of Simulation = 4 - Number of Epoch = 60\n",
      "Train loss:= 0.2723 - Val loss: 0.3317 - Test loss: 0.3469 - Train acc:= 91.84% - Val acc:= 89.92% - Test acc:= 89.54%\n",
      "Number of Simulation = 4 - Number of Epoch = 70\n",
      "Train loss:= 0.2414 - Val loss: 0.3094 - Test loss: 0.3218 - Train acc:= 92.83% - Val acc:= 90.73% - Test acc:= 90.37%\n",
      "Number of Simulation = 4 - Number of Epoch = 80\n",
      "Train loss:= 0.2159 - Val loss: 0.2908 - Test loss: 0.3015 - Train acc:= 93.65% - Val acc:= 91.39% - Test acc:= 90.91%\n",
      "Number of Simulation = 4 - Number of Epoch = 90\n",
      "Train loss:= 0.1945 - Val loss: 0.2747 - Test loss: 0.2845 - Train acc:= 94.38% - Val acc:= 91.88% - Test acc:= 91.39%\n",
      "Number of Simulation = 4 - Number of Epoch = 100\n",
      "Train loss:= 0.1762 - Val loss: 0.2607 - Test loss: 0.2700 - Train acc:= 94.99% - Val acc:= 92.35% - Test acc:= 92.04%\n",
      "Number of Simulation = 4 - Number of Epoch = 110\n",
      "Train loss:= 0.1604 - Val loss: 0.2481 - Test loss: 0.2571 - Train acc:= 95.46% - Val acc:= 92.68% - Test acc:= 92.30%\n",
      "Number of Simulation = 4 - Number of Epoch = 120\n",
      "Train loss:= 0.1465 - Val loss: 0.2368 - Test loss: 0.2456 - Train acc:= 95.89% - Val acc:= 92.92% - Test acc:= 92.62%\n",
      "Number of Simulation = 4 - Number of Epoch = 130\n",
      "Train loss:= 0.1341 - Val loss: 0.2266 - Test loss: 0.2349 - Train acc:= 96.26% - Val acc:= 93.14% - Test acc:= 93.05%\n",
      "Number of Simulation = 4 - Number of Epoch = 140\n",
      "Train loss:= 0.1231 - Val loss: 0.2172 - Test loss: 0.2252 - Train acc:= 96.58% - Val acc:= 93.44% - Test acc:= 93.21%\n",
      "Number of Simulation = 4 - Number of Epoch = 150\n",
      "Train loss:= 0.1132 - Val loss: 0.2085 - Test loss: 0.2163 - Train acc:= 96.91% - Val acc:= 93.74% - Test acc:= 93.41%\n",
      "Number of Simulation = 4 - Number of Epoch = 160\n",
      "Train loss:= 0.1044 - Val loss: 0.2005 - Test loss: 0.2082 - Train acc:= 97.23% - Val acc:= 94.01% - Test acc:= 93.55%\n",
      "Number of Simulation = 4 - Number of Epoch = 170\n",
      "Train loss:= 0.0964 - Val loss: 0.1931 - Test loss: 0.2007 - Train acc:= 97.49% - Val acc:= 94.17% - Test acc:= 93.81%\n",
      "Number of Simulation = 4 - Number of Epoch = 180\n",
      "Train loss:= 0.0892 - Val loss: 0.1862 - Test loss: 0.1936 - Train acc:= 97.74% - Val acc:= 94.46% - Test acc:= 93.95%\n",
      "Number of Simulation = 4 - Number of Epoch = 190\n",
      "Train loss:= 0.0828 - Val loss: 0.1798 - Test loss: 0.1870 - Train acc:= 97.96% - Val acc:= 94.63% - Test acc:= 94.18%\n",
      "Number of Simulation = 4 - Number of Epoch = 200\n",
      "Train loss:= 0.0769 - Val loss: 0.1737 - Test loss: 0.1808 - Train acc:= 98.14% - Val acc:= 94.77% - Test acc:= 94.41%\n",
      "\n",
      "Total time taken (in seconds): 1279.65\n",
      "Number of Simulation = 5 - Number of Epoch = 10\n",
      "Train loss:= 0.8395 - Val loss: 0.8033 - Test loss: 0.8512 - Train acc:= 74.99% - Val acc:= 76.36% - Test acc:= 75.14%\n",
      "Number of Simulation = 5 - Number of Epoch = 20\n",
      "Train loss:= 0.5620 - Val loss: 0.5635 - Test loss: 0.5977 - Train acc:= 83.23% - Val acc:= 83.58% - Test acc:= 82.24%\n",
      "Number of Simulation = 5 - Number of Epoch = 30\n",
      "Train loss:= 0.4421 - Val loss: 0.4645 - Test loss: 0.4922 - Train acc:= 86.84% - Val acc:= 86.53% - Test acc:= 85.26%\n",
      "Number of Simulation = 5 - Number of Epoch = 40\n",
      "Train loss:= 0.3677 - Val loss: 0.4055 - Test loss: 0.4301 - Train acc:= 89.08% - Val acc:= 88.10% - Test acc:= 87.21%\n",
      "Number of Simulation = 5 - Number of Epoch = 50\n",
      "Train loss:= 0.3150 - Val loss: 0.3655 - Test loss: 0.3876 - Train acc:= 90.61% - Val acc:= 89.30% - Test acc:= 88.41%\n",
      "Number of Simulation = 5 - Number of Epoch = 60\n",
      "Train loss:= 0.2747 - Val loss: 0.3359 - Test loss: 0.3563 - Train acc:= 91.86% - Val acc:= 90.32% - Test acc:= 89.49%\n",
      "Number of Simulation = 5 - Number of Epoch = 70\n",
      "Train loss:= 0.2424 - Val loss: 0.3126 - Test loss: 0.3319 - Train acc:= 92.81% - Val acc:= 91.01% - Test acc:= 90.08%\n",
      "Number of Simulation = 5 - Number of Epoch = 80\n",
      "Train loss:= 0.2158 - Val loss: 0.2933 - Test loss: 0.3123 - Train acc:= 93.65% - Val acc:= 91.52% - Test acc:= 90.63%\n",
      "Number of Simulation = 5 - Number of Epoch = 90\n",
      "Train loss:= 0.1934 - Val loss: 0.2770 - Test loss: 0.2956 - Train acc:= 94.36% - Val acc:= 92.02% - Test acc:= 91.24%\n",
      "Number of Simulation = 5 - Number of Epoch = 100\n",
      "Train loss:= 0.1746 - Val loss: 0.2630 - Test loss: 0.2813 - Train acc:= 94.96% - Val acc:= 92.36% - Test acc:= 91.79%\n",
      "Number of Simulation = 5 - Number of Epoch = 110\n",
      "Train loss:= 0.1584 - Val loss: 0.2507 - Test loss: 0.2688 - Train acc:= 95.51% - Val acc:= 92.78% - Test acc:= 92.20%\n",
      "Number of Simulation = 5 - Number of Epoch = 120\n",
      "Train loss:= 0.1443 - Val loss: 0.2398 - Test loss: 0.2575 - Train acc:= 95.96% - Val acc:= 93.10% - Test acc:= 92.50%\n",
      "Number of Simulation = 5 - Number of Epoch = 130\n",
      "Train loss:= 0.1318 - Val loss: 0.2301 - Test loss: 0.2473 - Train acc:= 96.38% - Val acc:= 93.39% - Test acc:= 92.71%\n",
      "Number of Simulation = 5 - Number of Epoch = 140\n",
      "Train loss:= 0.1207 - Val loss: 0.2212 - Test loss: 0.2379 - Train acc:= 96.72% - Val acc:= 93.60% - Test acc:= 92.86%\n",
      "Number of Simulation = 5 - Number of Epoch = 150\n",
      "Train loss:= 0.1109 - Val loss: 0.2132 - Test loss: 0.2292 - Train acc:= 97.02% - Val acc:= 93.77% - Test acc:= 93.08%\n",
      "Number of Simulation = 5 - Number of Epoch = 160\n",
      "Train loss:= 0.1021 - Val loss: 0.2057 - Test loss: 0.2209 - Train acc:= 97.34% - Val acc:= 94.00% - Test acc:= 93.29%\n",
      "Number of Simulation = 5 - Number of Epoch = 170\n",
      "Train loss:= 0.0941 - Val loss: 0.1987 - Test loss: 0.2131 - Train acc:= 97.59% - Val acc:= 94.16% - Test acc:= 93.46%\n",
      "Number of Simulation = 5 - Number of Epoch = 180\n",
      "Train loss:= 0.0870 - Val loss: 0.1921 - Test loss: 0.2056 - Train acc:= 97.83% - Val acc:= 94.37% - Test acc:= 93.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 5 - Number of Epoch = 190\n",
      "Train loss:= 0.0805 - Val loss: 0.1858 - Test loss: 0.1984 - Train acc:= 98.05% - Val acc:= 94.55% - Test acc:= 93.91%\n",
      "Number of Simulation = 5 - Number of Epoch = 200\n",
      "Train loss:= 0.0746 - Val loss: 0.1799 - Test loss: 0.1915 - Train acc:= 98.23% - Val acc:= 94.62% - Test acc:= 94.13%\n",
      "\n",
      "Total time taken (in seconds): 1290.75\n",
      "Number of Simulation = 6 - Number of Epoch = 10\n",
      "Train loss:= 0.8068 - Val loss: 0.7791 - Test loss: 0.7797 - Train acc:= 75.78% - Val acc:= 76.64% - Test acc:= 76.22%\n",
      "Number of Simulation = 6 - Number of Epoch = 20\n",
      "Train loss:= 0.5402 - Val loss: 0.5444 - Test loss: 0.5482 - Train acc:= 83.58% - Val acc:= 83.80% - Test acc:= 83.19%\n",
      "Number of Simulation = 6 - Number of Epoch = 30\n",
      "Train loss:= 0.4257 - Val loss: 0.4486 - Test loss: 0.4528 - Train acc:= 86.98% - Val acc:= 86.51% - Test acc:= 86.38%\n",
      "Number of Simulation = 6 - Number of Epoch = 40\n",
      "Train loss:= 0.3534 - Val loss: 0.3903 - Test loss: 0.3960 - Train acc:= 89.26% - Val acc:= 88.26% - Test acc:= 88.17%\n",
      "Number of Simulation = 6 - Number of Epoch = 50\n",
      "Train loss:= 0.3026 - Val loss: 0.3502 - Test loss: 0.3573 - Train acc:= 90.90% - Val acc:= 89.58% - Test acc:= 89.08%\n",
      "Number of Simulation = 6 - Number of Epoch = 60\n",
      "Train loss:= 0.2640 - Val loss: 0.3202 - Test loss: 0.3290 - Train acc:= 92.08% - Val acc:= 90.58% - Test acc:= 89.99%\n",
      "Number of Simulation = 6 - Number of Epoch = 70\n",
      "Train loss:= 0.2333 - Val loss: 0.2968 - Test loss: 0.3070 - Train acc:= 92.99% - Val acc:= 91.05% - Test acc:= 90.68%\n",
      "Number of Simulation = 6 - Number of Epoch = 80\n",
      "Train loss:= 0.2082 - Val loss: 0.2779 - Test loss: 0.2890 - Train acc:= 93.82% - Val acc:= 91.68% - Test acc:= 91.24%\n",
      "Number of Simulation = 6 - Number of Epoch = 90\n",
      "Train loss:= 0.1872 - Val loss: 0.2618 - Test loss: 0.2737 - Train acc:= 94.53% - Val acc:= 92.20% - Test acc:= 91.55%\n",
      "Number of Simulation = 6 - Number of Epoch = 100\n",
      "Train loss:= 0.1691 - Val loss: 0.2479 - Test loss: 0.2603 - Train acc:= 95.13% - Val acc:= 92.68% - Test acc:= 91.93%\n",
      "Number of Simulation = 6 - Number of Epoch = 110\n",
      "Train loss:= 0.1536 - Val loss: 0.2356 - Test loss: 0.2484 - Train acc:= 95.62% - Val acc:= 93.09% - Test acc:= 92.43%\n",
      "Number of Simulation = 6 - Number of Epoch = 120\n",
      "Train loss:= 0.1401 - Val loss: 0.2247 - Test loss: 0.2376 - Train acc:= 96.03% - Val acc:= 93.52% - Test acc:= 92.72%\n",
      "Number of Simulation = 6 - Number of Epoch = 130\n",
      "Train loss:= 0.1282 - Val loss: 0.2148 - Test loss: 0.2277 - Train acc:= 96.42% - Val acc:= 93.83% - Test acc:= 93.16%\n",
      "Number of Simulation = 6 - Number of Epoch = 140\n",
      "Train loss:= 0.1177 - Val loss: 0.2058 - Test loss: 0.2186 - Train acc:= 96.79% - Val acc:= 94.05% - Test acc:= 93.38%\n",
      "Number of Simulation = 6 - Number of Epoch = 150\n",
      "Train loss:= 0.1083 - Val loss: 0.1976 - Test loss: 0.2101 - Train acc:= 97.11% - Val acc:= 94.27% - Test acc:= 93.58%\n",
      "Number of Simulation = 6 - Number of Epoch = 160\n",
      "Train loss:= 0.0998 - Val loss: 0.1900 - Test loss: 0.2021 - Train acc:= 97.37% - Val acc:= 94.54% - Test acc:= 93.95%\n",
      "Number of Simulation = 6 - Number of Epoch = 170\n",
      "Train loss:= 0.0923 - Val loss: 0.1829 - Test loss: 0.1945 - Train acc:= 97.62% - Val acc:= 94.72% - Test acc:= 94.14%\n",
      "Number of Simulation = 6 - Number of Epoch = 180\n",
      "Train loss:= 0.0855 - Val loss: 0.1763 - Test loss: 0.1874 - Train acc:= 97.85% - Val acc:= 94.90% - Test acc:= 94.41%\n",
      "Number of Simulation = 6 - Number of Epoch = 190\n",
      "Train loss:= 0.0793 - Val loss: 0.1701 - Test loss: 0.1806 - Train acc:= 98.05% - Val acc:= 95.12% - Test acc:= 94.61%\n",
      "Number of Simulation = 6 - Number of Epoch = 200\n",
      "Train loss:= 0.0738 - Val loss: 0.1643 - Test loss: 0.1742 - Train acc:= 98.22% - Val acc:= 95.24% - Test acc:= 94.78%\n",
      "\n",
      "Total time taken (in seconds): 1292.39\n",
      "Number of Simulation = 7 - Number of Epoch = 10\n",
      "Train loss:= 0.8124 - Val loss: 0.8067 - Test loss: 0.8075 - Train acc:= 76.61% - Val acc:= 76.97% - Test acc:= 76.71%\n",
      "Number of Simulation = 7 - Number of Epoch = 20\n",
      "Train loss:= 0.5304 - Val loss: 0.5591 - Test loss: 0.5581 - Train acc:= 84.27% - Val acc:= 83.99% - Test acc:= 83.80%\n",
      "Number of Simulation = 7 - Number of Epoch = 30\n",
      "Train loss:= 0.4159 - Val loss: 0.4628 - Test loss: 0.4616 - Train acc:= 87.46% - Val acc:= 86.57% - Test acc:= 86.42%\n",
      "Number of Simulation = 7 - Number of Epoch = 40\n",
      "Train loss:= 0.3458 - Val loss: 0.4068 - Test loss: 0.4043 - Train acc:= 89.48% - Val acc:= 88.28% - Test acc:= 88.20%\n",
      "Number of Simulation = 7 - Number of Epoch = 50\n",
      "Train loss:= 0.2958 - Val loss: 0.3684 - Test loss: 0.3659 - Train acc:= 91.07% - Val acc:= 89.64% - Test acc:= 89.35%\n",
      "Number of Simulation = 7 - Number of Epoch = 60\n",
      "Train loss:= 0.2577 - Val loss: 0.3400 - Test loss: 0.3380 - Train acc:= 92.25% - Val acc:= 90.43% - Test acc:= 89.98%\n",
      "Number of Simulation = 7 - Number of Epoch = 70\n",
      "Train loss:= 0.2274 - Val loss: 0.3176 - Test loss: 0.3157 - Train acc:= 93.27% - Val acc:= 91.06% - Test acc:= 90.61%\n",
      "Number of Simulation = 7 - Number of Epoch = 80\n",
      "Train loss:= 0.2026 - Val loss: 0.2992 - Test loss: 0.2971 - Train acc:= 94.03% - Val acc:= 91.71% - Test acc:= 90.99%\n",
      "Number of Simulation = 7 - Number of Epoch = 90\n",
      "Train loss:= 0.1818 - Val loss: 0.2835 - Test loss: 0.2810 - Train acc:= 94.72% - Val acc:= 92.03% - Test acc:= 91.46%\n",
      "Number of Simulation = 7 - Number of Epoch = 100\n",
      "Train loss:= 0.1642 - Val loss: 0.2699 - Test loss: 0.2670 - Train acc:= 95.29% - Val acc:= 92.37% - Test acc:= 91.88%\n",
      "Number of Simulation = 7 - Number of Epoch = 110\n",
      "Train loss:= 0.1490 - Val loss: 0.2579 - Test loss: 0.2547 - Train acc:= 95.76% - Val acc:= 92.74% - Test acc:= 92.26%\n",
      "Number of Simulation = 7 - Number of Epoch = 120\n",
      "Train loss:= 0.1358 - Val loss: 0.2472 - Test loss: 0.2437 - Train acc:= 96.16% - Val acc:= 92.97% - Test acc:= 92.62%\n",
      "Number of Simulation = 7 - Number of Epoch = 130\n",
      "Train loss:= 0.1242 - Val loss: 0.2376 - Test loss: 0.2337 - Train acc:= 96.56% - Val acc:= 93.30% - Test acc:= 92.91%\n",
      "Number of Simulation = 7 - Number of Epoch = 140\n",
      "Train loss:= 0.1140 - Val loss: 0.2288 - Test loss: 0.2246 - Train acc:= 96.89% - Val acc:= 93.53% - Test acc:= 93.16%\n",
      "Number of Simulation = 7 - Number of Epoch = 150\n",
      "Train loss:= 0.1049 - Val loss: 0.2208 - Test loss: 0.2162 - Train acc:= 97.23% - Val acc:= 93.75% - Test acc:= 93.43%\n",
      "Number of Simulation = 7 - Number of Epoch = 160\n",
      "Train loss:= 0.0967 - Val loss: 0.2135 - Test loss: 0.2083 - Train acc:= 97.51% - Val acc:= 93.99% - Test acc:= 93.65%\n",
      "Number of Simulation = 7 - Number of Epoch = 170\n",
      "Train loss:= 0.0894 - Val loss: 0.2066 - Test loss: 0.2009 - Train acc:= 97.74% - Val acc:= 94.20% - Test acc:= 93.86%\n",
      "Number of Simulation = 7 - Number of Epoch = 180\n",
      "Train loss:= 0.0827 - Val loss: 0.2002 - Test loss: 0.1940 - Train acc:= 97.96% - Val acc:= 94.34% - Test acc:= 94.07%\n",
      "Number of Simulation = 7 - Number of Epoch = 190\n",
      "Train loss:= 0.0767 - Val loss: 0.1942 - Test loss: 0.1873 - Train acc:= 98.16% - Val acc:= 94.49% - Test acc:= 94.33%\n",
      "Number of Simulation = 7 - Number of Epoch = 200\n",
      "Train loss:= 0.0712 - Val loss: 0.1884 - Test loss: 0.1809 - Train acc:= 98.33% - Val acc:= 94.64% - Test acc:= 94.55%\n",
      "\n",
      "Total time taken (in seconds): 1292.09\n",
      "Number of Simulation = 8 - Number of Epoch = 10\n",
      "Train loss:= 0.8319 - Val loss: 0.8163 - Test loss: 0.8374 - Train acc:= 75.07% - Val acc:= 75.69% - Test acc:= 74.93%\n",
      "Number of Simulation = 8 - Number of Epoch = 20\n",
      "Train loss:= 0.5551 - Val loss: 0.5662 - Test loss: 0.5867 - Train acc:= 83.18% - Val acc:= 83.20% - Test acc:= 82.71%\n",
      "Number of Simulation = 8 - Number of Epoch = 30\n",
      "Train loss:= 0.4343 - Val loss: 0.4657 - Test loss: 0.4840 - Train acc:= 86.83% - Val acc:= 86.28% - Test acc:= 86.04%\n",
      "Number of Simulation = 8 - Number of Epoch = 40\n",
      "Train loss:= 0.3592 - Val loss: 0.4077 - Test loss: 0.4223 - Train acc:= 89.10% - Val acc:= 88.17% - Test acc:= 87.69%\n",
      "Number of Simulation = 8 - Number of Epoch = 50\n",
      "Train loss:= 0.3070 - Val loss: 0.3691 - Test loss: 0.3807 - Train acc:= 90.73% - Val acc:= 89.31% - Test acc:= 88.75%\n",
      "Number of Simulation = 8 - Number of Epoch = 60\n",
      "Train loss:= 0.2676 - Val loss: 0.3407 - Test loss: 0.3501 - Train acc:= 92.02% - Val acc:= 90.21% - Test acc:= 89.86%\n",
      "Number of Simulation = 8 - Number of Epoch = 70\n",
      "Train loss:= 0.2362 - Val loss: 0.3182 - Test loss: 0.3262 - Train acc:= 92.98% - Val acc:= 90.88% - Test acc:= 90.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 8 - Number of Epoch = 80\n",
      "Train loss:= 0.2104 - Val loss: 0.2993 - Test loss: 0.3067 - Train acc:= 93.74% - Val acc:= 91.28% - Test acc:= 91.17%\n",
      "Number of Simulation = 8 - Number of Epoch = 90\n",
      "Train loss:= 0.1887 - Val loss: 0.2828 - Test loss: 0.2901 - Train acc:= 94.39% - Val acc:= 91.70% - Test acc:= 91.64%\n",
      "Number of Simulation = 8 - Number of Epoch = 100\n",
      "Train loss:= 0.1702 - Val loss: 0.2683 - Test loss: 0.2754 - Train acc:= 95.04% - Val acc:= 92.09% - Test acc:= 92.18%\n",
      "Number of Simulation = 8 - Number of Epoch = 110\n",
      "Train loss:= 0.1543 - Val loss: 0.2555 - Test loss: 0.2625 - Train acc:= 95.55% - Val acc:= 92.58% - Test acc:= 92.52%\n",
      "Number of Simulation = 8 - Number of Epoch = 120\n",
      "Train loss:= 0.1405 - Val loss: 0.2440 - Test loss: 0.2509 - Train acc:= 95.94% - Val acc:= 92.83% - Test acc:= 92.91%\n",
      "Number of Simulation = 8 - Number of Epoch = 130\n",
      "Train loss:= 0.1285 - Val loss: 0.2336 - Test loss: 0.2404 - Train acc:= 96.37% - Val acc:= 93.03% - Test acc:= 93.19%\n",
      "Number of Simulation = 8 - Number of Epoch = 140\n",
      "Train loss:= 0.1179 - Val loss: 0.2241 - Test loss: 0.2308 - Train acc:= 96.76% - Val acc:= 93.36% - Test acc:= 93.47%\n",
      "Number of Simulation = 8 - Number of Epoch = 150\n",
      "Train loss:= 0.1084 - Val loss: 0.2155 - Test loss: 0.2219 - Train acc:= 97.05% - Val acc:= 93.64% - Test acc:= 93.72%\n",
      "Number of Simulation = 8 - Number of Epoch = 160\n",
      "Train loss:= 0.1000 - Val loss: 0.2074 - Test loss: 0.2136 - Train acc:= 97.36% - Val acc:= 94.03% - Test acc:= 93.96%\n",
      "Number of Simulation = 8 - Number of Epoch = 170\n",
      "Train loss:= 0.0924 - Val loss: 0.1998 - Test loss: 0.2059 - Train acc:= 97.64% - Val acc:= 94.16% - Test acc:= 94.05%\n",
      "Number of Simulation = 8 - Number of Epoch = 180\n",
      "Train loss:= 0.0855 - Val loss: 0.1926 - Test loss: 0.1985 - Train acc:= 97.87% - Val acc:= 94.28% - Test acc:= 94.23%\n",
      "Number of Simulation = 8 - Number of Epoch = 190\n",
      "Train loss:= 0.0793 - Val loss: 0.1858 - Test loss: 0.1914 - Train acc:= 98.07% - Val acc:= 94.49% - Test acc:= 94.31%\n",
      "Number of Simulation = 8 - Number of Epoch = 200\n",
      "Train loss:= 0.0737 - Val loss: 0.1792 - Test loss: 0.1847 - Train acc:= 98.27% - Val acc:= 94.70% - Test acc:= 94.49%\n",
      "\n",
      "Total time taken (in seconds): 1311.92\n",
      "Number of Simulation = 9 - Number of Epoch = 10\n",
      "Train loss:= 0.8277 - Val loss: 0.7978 - Test loss: 0.7964 - Train acc:= 75.49% - Val acc:= 76.53% - Test acc:= 75.69%\n",
      "Number of Simulation = 9 - Number of Epoch = 20\n",
      "Train loss:= 0.5553 - Val loss: 0.5662 - Test loss: 0.5636 - Train acc:= 83.14% - Val acc:= 83.21% - Test acc:= 82.67%\n",
      "Number of Simulation = 9 - Number of Epoch = 30\n",
      "Train loss:= 0.4334 - Val loss: 0.4663 - Test loss: 0.4695 - Train acc:= 86.86% - Val acc:= 85.96% - Test acc:= 85.61%\n",
      "Number of Simulation = 9 - Number of Epoch = 40\n",
      "Train loss:= 0.3587 - Val loss: 0.4065 - Test loss: 0.4142 - Train acc:= 89.10% - Val acc:= 88.06% - Test acc:= 87.08%\n",
      "Number of Simulation = 9 - Number of Epoch = 50\n",
      "Train loss:= 0.3052 - Val loss: 0.3643 - Test loss: 0.3748 - Train acc:= 90.67% - Val acc:= 89.17% - Test acc:= 88.42%\n",
      "Number of Simulation = 9 - Number of Epoch = 60\n",
      "Train loss:= 0.2642 - Val loss: 0.3325 - Test loss: 0.3445 - Train acc:= 91.93% - Val acc:= 90.19% - Test acc:= 89.51%\n",
      "Number of Simulation = 9 - Number of Epoch = 70\n",
      "Train loss:= 0.2317 - Val loss: 0.3071 - Test loss: 0.3201 - Train acc:= 92.97% - Val acc:= 91.09% - Test acc:= 90.33%\n",
      "Number of Simulation = 9 - Number of Epoch = 80\n",
      "Train loss:= 0.2053 - Val loss: 0.2867 - Test loss: 0.3002 - Train acc:= 93.83% - Val acc:= 91.77% - Test acc:= 90.93%\n",
      "Number of Simulation = 9 - Number of Epoch = 90\n",
      "Train loss:= 0.1834 - Val loss: 0.2700 - Test loss: 0.2834 - Train acc:= 94.49% - Val acc:= 92.17% - Test acc:= 91.39%\n",
      "Number of Simulation = 9 - Number of Epoch = 100\n",
      "Train loss:= 0.1649 - Val loss: 0.2561 - Test loss: 0.2687 - Train acc:= 95.11% - Val acc:= 92.55% - Test acc:= 92.00%\n",
      "Number of Simulation = 9 - Number of Epoch = 110\n",
      "Train loss:= 0.1490 - Val loss: 0.2442 - Test loss: 0.2557 - Train acc:= 95.62% - Val acc:= 93.00% - Test acc:= 92.31%\n",
      "Number of Simulation = 9 - Number of Epoch = 120\n",
      "Train loss:= 0.1351 - Val loss: 0.2338 - Test loss: 0.2439 - Train acc:= 96.06% - Val acc:= 93.19% - Test acc:= 92.69%\n",
      "Number of Simulation = 9 - Number of Epoch = 130\n",
      "Train loss:= 0.1229 - Val loss: 0.2245 - Test loss: 0.2332 - Train acc:= 96.50% - Val acc:= 93.45% - Test acc:= 92.99%\n",
      "Number of Simulation = 9 - Number of Epoch = 140\n",
      "Train loss:= 0.1121 - Val loss: 0.2159 - Test loss: 0.2234 - Train acc:= 96.90% - Val acc:= 93.67% - Test acc:= 93.27%\n",
      "Number of Simulation = 9 - Number of Epoch = 150\n",
      "Train loss:= 0.1026 - Val loss: 0.2080 - Test loss: 0.2144 - Train acc:= 97.23% - Val acc:= 93.92% - Test acc:= 93.47%\n",
      "Number of Simulation = 9 - Number of Epoch = 160\n",
      "Train loss:= 0.0940 - Val loss: 0.2006 - Test loss: 0.2061 - Train acc:= 97.48% - Val acc:= 94.16% - Test acc:= 93.75%\n",
      "Number of Simulation = 9 - Number of Epoch = 170\n",
      "Train loss:= 0.0864 - Val loss: 0.1936 - Test loss: 0.1985 - Train acc:= 97.73% - Val acc:= 94.38% - Test acc:= 94.07%\n",
      "Number of Simulation = 9 - Number of Epoch = 180\n",
      "Train loss:= 0.0796 - Val loss: 0.1870 - Test loss: 0.1913 - Train acc:= 97.96% - Val acc:= 94.62% - Test acc:= 94.24%\n",
      "Number of Simulation = 9 - Number of Epoch = 190\n",
      "Train loss:= 0.0734 - Val loss: 0.1807 - Test loss: 0.1846 - Train acc:= 98.15% - Val acc:= 94.79% - Test acc:= 94.41%\n",
      "Number of Simulation = 9 - Number of Epoch = 200\n",
      "Train loss:= 0.0678 - Val loss: 0.1747 - Test loss: 0.1782 - Train acc:= 98.36% - Val acc:= 94.99% - Test acc:= 94.57%\n",
      "\n",
      "Total time taken (in seconds): 1313.17\n",
      "Number of Simulation = 10 - Number of Epoch = 10\n",
      "Train loss:= 0.7987 - Val loss: 0.7731 - Test loss: 0.8075 - Train acc:= 75.81% - Val acc:= 76.66% - Test acc:= 75.96%\n",
      "Number of Simulation = 10 - Number of Epoch = 20\n",
      "Train loss:= 0.5378 - Val loss: 0.5433 - Test loss: 0.5708 - Train acc:= 83.56% - Val acc:= 83.65% - Test acc:= 82.96%\n",
      "Number of Simulation = 10 - Number of Epoch = 30\n",
      "Train loss:= 0.4241 - Val loss: 0.4525 - Test loss: 0.4733 - Train acc:= 87.07% - Val acc:= 86.76% - Test acc:= 86.11%\n",
      "Number of Simulation = 10 - Number of Epoch = 40\n",
      "Train loss:= 0.3538 - Val loss: 0.3993 - Test loss: 0.4157 - Train acc:= 89.28% - Val acc:= 88.34% - Test acc:= 87.98%\n",
      "Number of Simulation = 10 - Number of Epoch = 50\n",
      "Train loss:= 0.3036 - Val loss: 0.3617 - Test loss: 0.3759 - Train acc:= 90.83% - Val acc:= 89.48% - Test acc:= 88.88%\n",
      "Number of Simulation = 10 - Number of Epoch = 60\n",
      "Train loss:= 0.2650 - Val loss: 0.3326 - Test loss: 0.3461 - Train acc:= 91.93% - Val acc:= 90.28% - Test acc:= 89.67%\n",
      "Number of Simulation = 10 - Number of Epoch = 70\n",
      "Train loss:= 0.2339 - Val loss: 0.3087 - Test loss: 0.3225 - Train acc:= 92.99% - Val acc:= 90.92% - Test acc:= 90.52%\n",
      "Number of Simulation = 10 - Number of Epoch = 80\n",
      "Train loss:= 0.2082 - Val loss: 0.2888 - Test loss: 0.3031 - Train acc:= 93.81% - Val acc:= 91.63% - Test acc:= 91.19%\n",
      "Number of Simulation = 10 - Number of Epoch = 90\n",
      "Train loss:= 0.1867 - Val loss: 0.2718 - Test loss: 0.2864 - Train acc:= 94.55% - Val acc:= 92.12% - Test acc:= 91.66%\n",
      "Number of Simulation = 10 - Number of Epoch = 100\n",
      "Train loss:= 0.1683 - Val loss: 0.2571 - Test loss: 0.2718 - Train acc:= 95.13% - Val acc:= 92.49% - Test acc:= 92.06%\n",
      "Number of Simulation = 10 - Number of Epoch = 110\n",
      "Train loss:= 0.1525 - Val loss: 0.2443 - Test loss: 0.2586 - Train acc:= 95.61% - Val acc:= 92.90% - Test acc:= 92.46%\n",
      "Number of Simulation = 10 - Number of Epoch = 120\n",
      "Train loss:= 0.1387 - Val loss: 0.2330 - Test loss: 0.2468 - Train acc:= 96.06% - Val acc:= 93.25% - Test acc:= 92.79%\n",
      "Number of Simulation = 10 - Number of Epoch = 130\n",
      "Train loss:= 0.1265 - Val loss: 0.2229 - Test loss: 0.2361 - Train acc:= 96.54% - Val acc:= 93.44% - Test acc:= 93.13%\n",
      "Number of Simulation = 10 - Number of Epoch = 140\n",
      "Train loss:= 0.1157 - Val loss: 0.2139 - Test loss: 0.2263 - Train acc:= 96.86% - Val acc:= 93.73% - Test acc:= 93.32%\n",
      "Number of Simulation = 10 - Number of Epoch = 150\n",
      "Train loss:= 0.1060 - Val loss: 0.2057 - Test loss: 0.2173 - Train acc:= 97.18% - Val acc:= 94.00% - Test acc:= 93.63%\n",
      "Number of Simulation = 10 - Number of Epoch = 160\n",
      "Train loss:= 0.0974 - Val loss: 0.1981 - Test loss: 0.2091 - Train acc:= 97.50% - Val acc:= 94.31% - Test acc:= 93.84%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 10 - Number of Epoch = 170\n",
      "Train loss:= 0.0897 - Val loss: 0.1911 - Test loss: 0.2015 - Train acc:= 97.77% - Val acc:= 94.47% - Test acc:= 94.02%\n",
      "Number of Simulation = 10 - Number of Epoch = 180\n",
      "Train loss:= 0.0828 - Val loss: 0.1844 - Test loss: 0.1943 - Train acc:= 97.97% - Val acc:= 94.72% - Test acc:= 94.22%\n",
      "Number of Simulation = 10 - Number of Epoch = 190\n",
      "Train loss:= 0.0766 - Val loss: 0.1781 - Test loss: 0.1876 - Train acc:= 98.17% - Val acc:= 94.90% - Test acc:= 94.39%\n",
      "Number of Simulation = 10 - Number of Epoch = 200\n",
      "Train loss:= 0.0710 - Val loss: 0.1721 - Test loss: 0.1812 - Train acc:= 98.34% - Val acc:= 95.08% - Test acc:= 94.57%\n",
      "\n",
      "Total time taken (in seconds): 1332.54\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "    #mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1.0, 'lr':1e-4}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs)\n",
    "\n",
    "            #use adam to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams)\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final average result: test loss := 0.1817 - test acc := 94.54% - the std of test acc := 0.0449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_average_loss = np.mean(train_loss_record,0)\n",
    "train_average_acc = np.mean(train_acc_record,0)\n",
    "val_average_loss = np.mean(val_loss_record,0)\n",
    "val_average_acc = np.mean(val_acc_record,0)\n",
    "test_average_loss = np.mean(test_loss_record,0)\n",
    "test_average_acc = np.mean(test_acc_record,0)\n",
    "\n",
    "test_var_acc = np.var(test_acc_record,1)[-1]\n",
    "x = np.linspace(1,NUM_EPOCHS,len(train_average_loss))\n",
    "\n",
    "plt.figure()\n",
    "f, ax = plt.subplots(1,2, sharex = True, sharey = False,figsize=(8,3))\n",
    "f.suptitle('Report of MLP with l2 regularizer for MNIST digits data set')\n",
    "\n",
    "\n",
    "ax[0].plot(x,train_average_loss,label='train')\n",
    "ax[0].plot(x,val_average_loss,label='val')\n",
    "ax[0].plot(x,test_average_loss,label='test')\n",
    "#ax[0].set_yscale('log')\n",
    "ax[0].set_title('')\n",
    "ax.flat[0].set(xlabel='Num of epochs', ylabel='Average loss')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(x,train_average_acc,label='train')\n",
    "ax[1].plot(x,val_average_acc,label='val')\n",
    "ax[1].plot(x,test_average_acc,label='test')\n",
    "ax[1].set_title('')\n",
    "ax.flat[1].set(xlabel='Num of epochs', ylabel='Average accuray')\n",
    "ax[1].legend()\n",
    "\n",
    "print('the final average result: test loss := {:.4f} - test acc := {:.2%} - the std of test acc := {:.4f}'\n",
    "      .format(test_average_loss[-1], test_average_acc[-1], np.sqrt(test_var_acc)))\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
