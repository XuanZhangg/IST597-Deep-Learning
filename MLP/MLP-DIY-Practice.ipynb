{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: (50000, 784)\n",
      "val_size: (10000, 784)\n",
      "test_size: (10000, 784)\n",
      "train_output_size (50000,)\n",
      "val_output_size: (10000,)\n",
      "test_output_size (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load and normalize data\n",
    "#(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = tf.reshape(X_train, (X_train.shape[0],-1))/255\n",
    "X_test = tf.reshape(X_test, (X_test.shape[0],-1))/255\n",
    "\n",
    "#reserve the last 10000 training examples for validation\n",
    "X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "print(\"train_size:\", X_train.shape)\n",
    "print(\"val_size:\", X_val.shape)\n",
    "print(\"test_size:\", X_test.shape)\n",
    "print(\"train_output_size\", y_train.shape)\n",
    "print(\"val_output_size:\", y_val.shape)\n",
    "print(\"test_output_size\", y_test.shape)\n",
    "\n",
    "size_input = X_train.shape[1]\n",
    "size_output = len(set(y_train))\n",
    "size_hidden1 = 256\n",
    "size_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        regularizer: str or None\n",
    "        R_lambda: the parameter for regularizer\n",
    "        drop_prob: 0 to 1\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_output, device\n",
    "        \n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "        \n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
    "        # Initialize biases for hidden layer 1\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights between hidden layer 1 and hidden layer 2\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
    "        # Initialize biases for hidden layer 2\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "\n",
    "         # Initialize weights between hidden layer 2 and output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "        # Initialize the state of Adam-yogi algorithm\n",
    "        self.v_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.v_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.s_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.s_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        \n",
    "        self.v_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.v_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.s_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.s_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        \n",
    "        self.v_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.v_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.s_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.s_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        \n",
    "        self.v_state = [self.v_W1,self.v_W2,self.v_W3,self.v_b1,self.v_b2,self.v_b3]\n",
    "        self.s_state = [self.s_W1,self.s_W2,self.s_W3,self.s_b1,self.s_b2,self.s_b3]\n",
    "         \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        def compute_output(X):\n",
    "            # Cast X to float32\n",
    "            X_tf = tf.cast(X, dtype=tf.float32)\n",
    "            \n",
    "            #set the dropout prob\n",
    "            prob = self.drop_prob\n",
    "\n",
    "            # Remember to normalize your dataset before moving forward\n",
    "            # Compute values in hidden layer 1\n",
    "            what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "            hhat1 = tf.nn.experimental.stateless_dropout(tf.nn.sigmoid(what1), rate = prob, seed = [1,0])\n",
    "\n",
    "            # Compute values in hidden layer 2\n",
    "            what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "            hhat2 = tf.nn.experimental.stateless_dropout(tf.nn.sigmoid(what2), rate = prob, seed = [1,0])\n",
    "\n",
    "            # Compute output\n",
    "            output = tf.nn.softmax(tf.matmul(hhat2, self.W3) + self.b3)\n",
    "\n",
    "            return output\n",
    "        \n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = compute_output(X)\n",
    "        else:\n",
    "            self.y = compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true,y_pred, from_logits = False)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "        \n",
    "    def backward(self, X_train, y_train, hyperparams):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "\n",
    "        #use adam optimizer\n",
    "        '''\n",
    "        adam optimizer\n",
    "        '''\n",
    "        beta1, beta2, eps = 0.9, 0.999, 1e-6\n",
    "\n",
    "        for p, v, s, grad in zip(self.variables, self.v_state, self.s_state, grads):\n",
    "            v[:].assign(beta1 * v  + (1 - beta1) * grad)\n",
    "            s[:].assign(s + (1 - beta2) * tf.math.sign(tf.math.square(grad) - s) * tf.math.square(grad))\n",
    "            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
    "            p[:].assign(p - hyperparams['lr'] * v_bias_corr/ (tf.math.sqrt(s_bias_corr) + eps))\n",
    "            \n",
    "\n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))\n",
    "    \n",
    "#     def dropout_layer(self,X, dropout):\n",
    "#         assert 0 <= dropout <= 1\n",
    "#         # In this case, all elements are dropped out\n",
    "#         if dropout == 1:\n",
    "#             return tf.zeros_like(X)\n",
    "#         # In this case, all elements are kept\n",
    "#         if dropout == 0:\n",
    "#             return X\n",
    "#         mask = tf.random.uniform(\n",
    "#             shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\n",
    "#         return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.7107 - Val loss: 0.7438 - Test loss: 0.7818 - Train acc:= 76.41% - Val acc:= 74.98% - Test acc:= 74.62%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.5814 - Val loss: 0.6307 - Test loss: 0.6664 - Train acc:= 80.05% - Val acc:= 78.14% - Test acc:= 78.05%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.5145 - Val loss: 0.5733 - Test loss: 0.6121 - Train acc:= 82.14% - Val acc:= 79.70% - Test acc:= 79.78%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.4697 - Val loss: 0.5375 - Test loss: 0.5774 - Train acc:= 83.57% - Val acc:= 81.16% - Test acc:= 80.51%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.4360 - Val loss: 0.5123 - Test loss: 0.5529 - Train acc:= 84.62% - Val acc:= 81.97% - Test acc:= 81.29%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.4086 - Val loss: 0.4926 - Test loss: 0.5329 - Train acc:= 85.52% - Val acc:= 82.61% - Test acc:= 82.04%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.3856 - Val loss: 0.4763 - Test loss: 0.5164 - Train acc:= 86.29% - Val acc:= 83.26% - Test acc:= 82.51%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.3660 - Val loss: 0.4625 - Test loss: 0.5022 - Train acc:= 86.97% - Val acc:= 83.60% - Test acc:= 82.82%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.3488 - Val loss: 0.4505 - Test loss: 0.4896 - Train acc:= 87.57% - Val acc:= 83.89% - Test acc:= 83.14%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.3334 - Val loss: 0.4400 - Test loss: 0.4782 - Train acc:= 88.04% - Val acc:= 84.26% - Test acc:= 83.31%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.3195 - Val loss: 0.4306 - Test loss: 0.4677 - Train acc:= 88.61% - Val acc:= 84.61% - Test acc:= 83.64%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.3067 - Val loss: 0.4223 - Test loss: 0.4580 - Train acc:= 89.10% - Val acc:= 84.97% - Test acc:= 83.98%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.2950 - Val loss: 0.4147 - Test loss: 0.4490 - Train acc:= 89.55% - Val acc:= 85.19% - Test acc:= 84.33%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.2842 - Val loss: 0.4079 - Test loss: 0.4409 - Train acc:= 89.89% - Val acc:= 85.52% - Test acc:= 84.54%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.2742 - Val loss: 0.4016 - Test loss: 0.4336 - Train acc:= 90.25% - Val acc:= 85.63% - Test acc:= 84.81%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.2650 - Val loss: 0.3958 - Test loss: 0.4269 - Train acc:= 90.63% - Val acc:= 85.85% - Test acc:= 85.05%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.2563 - Val loss: 0.3905 - Test loss: 0.4208 - Train acc:= 91.02% - Val acc:= 86.12% - Test acc:= 85.27%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.2482 - Val loss: 0.3856 - Test loss: 0.4152 - Train acc:= 91.33% - Val acc:= 86.25% - Test acc:= 85.41%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.2405 - Val loss: 0.3810 - Test loss: 0.4101 - Train acc:= 91.59% - Val acc:= 86.35% - Test acc:= 85.60%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.2334 - Val loss: 0.3767 - Test loss: 0.4053 - Train acc:= 91.87% - Val acc:= 86.44% - Test acc:= 85.65%\n",
      "\n",
      "Total time taken (in seconds): 1264.77\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 0.7420 - Val loss: 0.7823 - Test loss: 0.8161 - Train acc:= 75.46% - Val acc:= 73.82% - Test acc:= 73.61%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 0.6027 - Val loss: 0.6606 - Test loss: 0.6937 - Train acc:= 79.31% - Val acc:= 77.03% - Test acc:= 77.06%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 0.5312 - Val loss: 0.5983 - Test loss: 0.6346 - Train acc:= 81.63% - Val acc:= 78.93% - Test acc:= 78.66%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 0.4845 - Val loss: 0.5594 - Test loss: 0.5969 - Train acc:= 83.07% - Val acc:= 80.15% - Test acc:= 79.79%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 0.4489 - Val loss: 0.5310 - Test loss: 0.5689 - Train acc:= 84.26% - Val acc:= 81.09% - Test acc:= 80.79%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 0.4204 - Val loss: 0.5093 - Test loss: 0.5465 - Train acc:= 85.27% - Val acc:= 81.88% - Test acc:= 81.42%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 0.3964 - Val loss: 0.4912 - Test loss: 0.5279 - Train acc:= 86.03% - Val acc:= 82.57% - Test acc:= 81.92%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 0.3759 - Val loss: 0.4759 - Test loss: 0.5123 - Train acc:= 86.72% - Val acc:= 83.25% - Test acc:= 82.36%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 0.3581 - Val loss: 0.4624 - Test loss: 0.4988 - Train acc:= 87.37% - Val acc:= 83.81% - Test acc:= 82.89%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 0.3426 - Val loss: 0.4505 - Test loss: 0.4872 - Train acc:= 87.95% - Val acc:= 84.26% - Test acc:= 83.34%\n",
      "Number of Simulation = 2 - Number of Epoch = 110\n",
      "Train loss:= 0.3287 - Val loss: 0.4398 - Test loss: 0.4772 - Train acc:= 88.41% - Val acc:= 84.53% - Test acc:= 83.75%\n",
      "Number of Simulation = 2 - Number of Epoch = 120\n",
      "Train loss:= 0.3162 - Val loss: 0.4303 - Test loss: 0.4683 - Train acc:= 88.85% - Val acc:= 84.87% - Test acc:= 84.03%\n",
      "Number of Simulation = 2 - Number of Epoch = 130\n",
      "Train loss:= 0.3048 - Val loss: 0.4217 - Test loss: 0.4604 - Train acc:= 89.29% - Val acc:= 85.23% - Test acc:= 84.33%\n",
      "Number of Simulation = 2 - Number of Epoch = 140\n",
      "Train loss:= 0.2943 - Val loss: 0.4140 - Test loss: 0.4533 - Train acc:= 89.66% - Val acc:= 85.54% - Test acc:= 84.56%\n",
      "Number of Simulation = 2 - Number of Epoch = 150\n",
      "Train loss:= 0.2847 - Val loss: 0.4070 - Test loss: 0.4468 - Train acc:= 89.97% - Val acc:= 85.85% - Test acc:= 84.68%\n",
      "Number of Simulation = 2 - Number of Epoch = 160\n",
      "Train loss:= 0.2757 - Val loss: 0.4005 - Test loss: 0.4408 - Train acc:= 90.32% - Val acc:= 86.10% - Test acc:= 84.78%\n",
      "Number of Simulation = 2 - Number of Epoch = 170\n",
      "Train loss:= 0.2674 - Val loss: 0.3948 - Test loss: 0.4351 - Train acc:= 90.64% - Val acc:= 86.18% - Test acc:= 85.02%\n",
      "Number of Simulation = 2 - Number of Epoch = 180\n",
      "Train loss:= 0.2595 - Val loss: 0.3894 - Test loss: 0.4297 - Train acc:= 90.97% - Val acc:= 86.34% - Test acc:= 85.29%\n",
      "Number of Simulation = 2 - Number of Epoch = 190\n",
      "Train loss:= 0.2520 - Val loss: 0.3844 - Test loss: 0.4245 - Train acc:= 91.26% - Val acc:= 86.55% - Test acc:= 85.47%\n",
      "Number of Simulation = 2 - Number of Epoch = 200\n",
      "Train loss:= 0.2448 - Val loss: 0.3797 - Test loss: 0.4195 - Train acc:= 91.57% - Val acc:= 86.69% - Test acc:= 85.61%\n",
      "\n",
      "Total time taken (in seconds): 1263.40\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 0.7010 - Val loss: 0.7455 - Test loss: 0.7514 - Train acc:= 76.44% - Val acc:= 74.63% - Test acc:= 74.84%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 0.5743 - Val loss: 0.6315 - Test loss: 0.6485 - Train acc:= 80.26% - Val acc:= 78.07% - Test acc:= 77.92%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 0.5093 - Val loss: 0.5755 - Test loss: 0.5988 - Train acc:= 82.27% - Val acc:= 79.86% - Test acc:= 79.69%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 0.4640 - Val loss: 0.5380 - Test loss: 0.5641 - Train acc:= 83.65% - Val acc:= 81.14% - Test acc:= 80.36%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 0.4294 - Val loss: 0.5104 - Test loss: 0.5374 - Train acc:= 84.84% - Val acc:= 81.90% - Test acc:= 81.39%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 0.4022 - Val loss: 0.4902 - Test loss: 0.5175 - Train acc:= 85.75% - Val acc:= 82.64% - Test acc:= 81.98%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 0.3796 - Val loss: 0.4751 - Test loss: 0.5018 - Train acc:= 86.53% - Val acc:= 83.04% - Test acc:= 82.58%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 0.3604 - Val loss: 0.4626 - Test loss: 0.4886 - Train acc:= 87.19% - Val acc:= 83.46% - Test acc:= 83.07%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 0.3438 - Val loss: 0.4520 - Test loss: 0.4774 - Train acc:= 87.80% - Val acc:= 84.01% - Test acc:= 83.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 0.3291 - Val loss: 0.4427 - Test loss: 0.4675 - Train acc:= 88.27% - Val acc:= 84.27% - Test acc:= 83.81%\n",
      "Number of Simulation = 3 - Number of Epoch = 110\n",
      "Train loss:= 0.3159 - Val loss: 0.4343 - Test loss: 0.4585 - Train acc:= 88.76% - Val acc:= 84.67% - Test acc:= 84.21%\n",
      "Number of Simulation = 3 - Number of Epoch = 120\n",
      "Train loss:= 0.3039 - Val loss: 0.4267 - Test loss: 0.4504 - Train acc:= 89.24% - Val acc:= 85.10% - Test acc:= 84.47%\n",
      "Number of Simulation = 3 - Number of Epoch = 130\n",
      "Train loss:= 0.2930 - Val loss: 0.4197 - Test loss: 0.4429 - Train acc:= 89.65% - Val acc:= 85.29% - Test acc:= 84.66%\n",
      "Number of Simulation = 3 - Number of Epoch = 140\n",
      "Train loss:= 0.2830 - Val loss: 0.4133 - Test loss: 0.4361 - Train acc:= 90.04% - Val acc:= 85.48% - Test acc:= 84.98%\n",
      "Number of Simulation = 3 - Number of Epoch = 150\n",
      "Train loss:= 0.2737 - Val loss: 0.4074 - Test loss: 0.4297 - Train acc:= 90.36% - Val acc:= 85.79% - Test acc:= 85.08%\n",
      "Number of Simulation = 3 - Number of Epoch = 160\n",
      "Train loss:= 0.2650 - Val loss: 0.4018 - Test loss: 0.4237 - Train acc:= 90.74% - Val acc:= 86.00% - Test acc:= 85.31%\n",
      "Number of Simulation = 3 - Number of Epoch = 170\n",
      "Train loss:= 0.2569 - Val loss: 0.3966 - Test loss: 0.4181 - Train acc:= 91.06% - Val acc:= 86.11% - Test acc:= 85.54%\n",
      "Number of Simulation = 3 - Number of Epoch = 180\n",
      "Train loss:= 0.2493 - Val loss: 0.3916 - Test loss: 0.4127 - Train acc:= 91.33% - Val acc:= 86.21% - Test acc:= 85.74%\n",
      "Number of Simulation = 3 - Number of Epoch = 190\n",
      "Train loss:= 0.2421 - Val loss: 0.3868 - Test loss: 0.4076 - Train acc:= 91.63% - Val acc:= 86.38% - Test acc:= 85.93%\n",
      "Number of Simulation = 3 - Number of Epoch = 200\n",
      "Train loss:= 0.2353 - Val loss: 0.3821 - Test loss: 0.4028 - Train acc:= 91.85% - Val acc:= 86.53% - Test acc:= 86.00%\n",
      "\n",
      "Total time taken (in seconds): 1272.81\n",
      "Number of Simulation = 4 - Number of Epoch = 10\n",
      "Train loss:= 0.7064 - Val loss: 0.7498 - Test loss: 0.7750 - Train acc:= 76.30% - Val acc:= 74.44% - Test acc:= 74.50%\n",
      "Number of Simulation = 4 - Number of Epoch = 20\n",
      "Train loss:= 0.5739 - Val loss: 0.6362 - Test loss: 0.6616 - Train acc:= 80.12% - Val acc:= 77.88% - Test acc:= 78.13%\n",
      "Number of Simulation = 4 - Number of Epoch = 30\n",
      "Train loss:= 0.5071 - Val loss: 0.5813 - Test loss: 0.6091 - Train acc:= 82.25% - Val acc:= 80.03% - Test acc:= 79.48%\n",
      "Number of Simulation = 4 - Number of Epoch = 40\n",
      "Train loss:= 0.4638 - Val loss: 0.5457 - Test loss: 0.5764 - Train acc:= 83.68% - Val acc:= 81.10% - Test acc:= 80.44%\n",
      "Number of Simulation = 4 - Number of Epoch = 50\n",
      "Train loss:= 0.4314 - Val loss: 0.5207 - Test loss: 0.5529 - Train acc:= 84.86% - Val acc:= 81.72% - Test acc:= 81.31%\n",
      "Number of Simulation = 4 - Number of Epoch = 60\n",
      "Train loss:= 0.4054 - Val loss: 0.5012 - Test loss: 0.5337 - Train acc:= 85.74% - Val acc:= 82.49% - Test acc:= 81.92%\n",
      "Number of Simulation = 4 - Number of Epoch = 70\n",
      "Train loss:= 0.3835 - Val loss: 0.4850 - Test loss: 0.5178 - Train acc:= 86.49% - Val acc:= 82.87% - Test acc:= 82.27%\n",
      "Number of Simulation = 4 - Number of Epoch = 80\n",
      "Train loss:= 0.3645 - Val loss: 0.4711 - Test loss: 0.5046 - Train acc:= 87.08% - Val acc:= 83.22% - Test acc:= 82.55%\n",
      "Number of Simulation = 4 - Number of Epoch = 90\n",
      "Train loss:= 0.3478 - Val loss: 0.4591 - Test loss: 0.4934 - Train acc:= 87.64% - Val acc:= 83.74% - Test acc:= 83.05%\n",
      "Number of Simulation = 4 - Number of Epoch = 100\n",
      "Train loss:= 0.3329 - Val loss: 0.4485 - Test loss: 0.4835 - Train acc:= 88.16% - Val acc:= 84.22% - Test acc:= 83.44%\n",
      "Number of Simulation = 4 - Number of Epoch = 110\n",
      "Train loss:= 0.3194 - Val loss: 0.4392 - Test loss: 0.4745 - Train acc:= 88.67% - Val acc:= 84.59% - Test acc:= 83.73%\n",
      "Number of Simulation = 4 - Number of Epoch = 120\n",
      "Train loss:= 0.3073 - Val loss: 0.4308 - Test loss: 0.4662 - Train acc:= 89.06% - Val acc:= 84.85% - Test acc:= 84.02%\n",
      "Number of Simulation = 4 - Number of Epoch = 130\n",
      "Train loss:= 0.2961 - Val loss: 0.4231 - Test loss: 0.4587 - Train acc:= 89.49% - Val acc:= 85.02% - Test acc:= 84.24%\n",
      "Number of Simulation = 4 - Number of Epoch = 140\n",
      "Train loss:= 0.2858 - Val loss: 0.4161 - Test loss: 0.4517 - Train acc:= 89.86% - Val acc:= 85.34% - Test acc:= 84.44%\n",
      "Number of Simulation = 4 - Number of Epoch = 150\n",
      "Train loss:= 0.2762 - Val loss: 0.4096 - Test loss: 0.4453 - Train acc:= 90.24% - Val acc:= 85.58% - Test acc:= 84.63%\n",
      "Number of Simulation = 4 - Number of Epoch = 160\n",
      "Train loss:= 0.2673 - Val loss: 0.4036 - Test loss: 0.4394 - Train acc:= 90.59% - Val acc:= 85.78% - Test acc:= 84.85%\n",
      "Number of Simulation = 4 - Number of Epoch = 170\n",
      "Train loss:= 0.2590 - Val loss: 0.3980 - Test loss: 0.4339 - Train acc:= 90.89% - Val acc:= 86.05% - Test acc:= 84.99%\n",
      "Number of Simulation = 4 - Number of Epoch = 180\n",
      "Train loss:= 0.2511 - Val loss: 0.3928 - Test loss: 0.4286 - Train acc:= 91.24% - Val acc:= 86.20% - Test acc:= 85.18%\n",
      "Number of Simulation = 4 - Number of Epoch = 190\n",
      "Train loss:= 0.2438 - Val loss: 0.3878 - Test loss: 0.4235 - Train acc:= 91.52% - Val acc:= 86.40% - Test acc:= 85.39%\n",
      "Number of Simulation = 4 - Number of Epoch = 200\n",
      "Train loss:= 0.2369 - Val loss: 0.3831 - Test loss: 0.4186 - Train acc:= 91.86% - Val acc:= 86.64% - Test acc:= 85.59%\n",
      "\n",
      "Total time taken (in seconds): 1279.87\n",
      "Number of Simulation = 5 - Number of Epoch = 10\n",
      "Train loss:= 0.7412 - Val loss: 0.7686 - Test loss: 0.7981 - Train acc:= 75.22% - Val acc:= 74.25% - Test acc:= 73.31%\n",
      "Number of Simulation = 5 - Number of Epoch = 20\n",
      "Train loss:= 0.6014 - Val loss: 0.6455 - Test loss: 0.6710 - Train acc:= 79.35% - Val acc:= 77.54% - Test acc:= 77.23%\n",
      "Number of Simulation = 5 - Number of Epoch = 30\n",
      "Train loss:= 0.5317 - Val loss: 0.5868 - Test loss: 0.6125 - Train acc:= 81.53% - Val acc:= 79.27% - Test acc:= 79.05%\n",
      "Number of Simulation = 5 - Number of Epoch = 40\n",
      "Train loss:= 0.4848 - Val loss: 0.5487 - Test loss: 0.5746 - Train acc:= 83.02% - Val acc:= 80.66% - Test acc:= 80.25%\n",
      "Number of Simulation = 5 - Number of Epoch = 50\n",
      "Train loss:= 0.4487 - Val loss: 0.5205 - Test loss: 0.5473 - Train acc:= 84.20% - Val acc:= 81.66% - Test acc:= 81.24%\n",
      "Number of Simulation = 5 - Number of Epoch = 60\n",
      "Train loss:= 0.4199 - Val loss: 0.4991 - Test loss: 0.5269 - Train acc:= 85.18% - Val acc:= 82.36% - Test acc:= 82.08%\n",
      "Number of Simulation = 5 - Number of Epoch = 70\n",
      "Train loss:= 0.3959 - Val loss: 0.4819 - Test loss: 0.5106 - Train acc:= 85.98% - Val acc:= 83.07% - Test acc:= 82.54%\n",
      "Number of Simulation = 5 - Number of Epoch = 80\n",
      "Train loss:= 0.3754 - Val loss: 0.4679 - Test loss: 0.4969 - Train acc:= 86.58% - Val acc:= 83.69% - Test acc:= 82.98%\n",
      "Number of Simulation = 5 - Number of Epoch = 90\n",
      "Train loss:= 0.3577 - Val loss: 0.4560 - Test loss: 0.4851 - Train acc:= 87.17% - Val acc:= 84.01% - Test acc:= 83.26%\n",
      "Number of Simulation = 5 - Number of Epoch = 100\n",
      "Train loss:= 0.3419 - Val loss: 0.4456 - Test loss: 0.4749 - Train acc:= 87.77% - Val acc:= 84.30% - Test acc:= 83.62%\n",
      "Number of Simulation = 5 - Number of Epoch = 110\n",
      "Train loss:= 0.3278 - Val loss: 0.4362 - Test loss: 0.4656 - Train acc:= 88.33% - Val acc:= 84.58% - Test acc:= 83.95%\n",
      "Number of Simulation = 5 - Number of Epoch = 120\n",
      "Train loss:= 0.3149 - Val loss: 0.4276 - Test loss: 0.4570 - Train acc:= 88.82% - Val acc:= 84.96% - Test acc:= 84.22%\n",
      "Number of Simulation = 5 - Number of Epoch = 130\n",
      "Train loss:= 0.3032 - Val loss: 0.4197 - Test loss: 0.4491 - Train acc:= 89.28% - Val acc:= 85.18% - Test acc:= 84.45%\n",
      "Number of Simulation = 5 - Number of Epoch = 140\n",
      "Train loss:= 0.2924 - Val loss: 0.4123 - Test loss: 0.4419 - Train acc:= 89.68% - Val acc:= 85.44% - Test acc:= 84.65%\n",
      "Number of Simulation = 5 - Number of Epoch = 150\n",
      "Train loss:= 0.2824 - Val loss: 0.4055 - Test loss: 0.4351 - Train acc:= 90.02% - Val acc:= 85.78% - Test acc:= 84.83%\n",
      "Number of Simulation = 5 - Number of Epoch = 160\n",
      "Train loss:= 0.2729 - Val loss: 0.3989 - Test loss: 0.4286 - Train acc:= 90.37% - Val acc:= 85.99% - Test acc:= 85.03%\n",
      "Number of Simulation = 5 - Number of Epoch = 170\n",
      "Train loss:= 0.2640 - Val loss: 0.3928 - Test loss: 0.4225 - Train acc:= 90.66% - Val acc:= 86.23% - Test acc:= 85.18%\n",
      "Number of Simulation = 5 - Number of Epoch = 180\n",
      "Train loss:= 0.2557 - Val loss: 0.3869 - Test loss: 0.4165 - Train acc:= 90.99% - Val acc:= 86.42% - Test acc:= 85.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 5 - Number of Epoch = 190\n",
      "Train loss:= 0.2478 - Val loss: 0.3814 - Test loss: 0.4108 - Train acc:= 91.31% - Val acc:= 86.64% - Test acc:= 85.58%\n",
      "Number of Simulation = 5 - Number of Epoch = 200\n",
      "Train loss:= 0.2404 - Val loss: 0.3763 - Test loss: 0.4054 - Train acc:= 91.57% - Val acc:= 86.89% - Test acc:= 85.72%\n",
      "\n",
      "Total time taken (in seconds): 1283.04\n",
      "Number of Simulation = 6 - Number of Epoch = 10\n",
      "Train loss:= 0.7116 - Val loss: 0.7293 - Test loss: 0.7639 - Train acc:= 76.15% - Val acc:= 74.91% - Test acc:= 74.72%\n",
      "Number of Simulation = 6 - Number of Epoch = 20\n",
      "Train loss:= 0.5845 - Val loss: 0.6229 - Test loss: 0.6601 - Train acc:= 79.74% - Val acc:= 78.25% - Test acc:= 77.65%\n",
      "Number of Simulation = 6 - Number of Epoch = 30\n",
      "Train loss:= 0.5208 - Val loss: 0.5725 - Test loss: 0.6105 - Train acc:= 81.82% - Val acc:= 79.99% - Test acc:= 79.30%\n",
      "Number of Simulation = 6 - Number of Epoch = 40\n",
      "Train loss:= 0.4766 - Val loss: 0.5390 - Test loss: 0.5764 - Train acc:= 83.18% - Val acc:= 80.88% - Test acc:= 80.29%\n",
      "Number of Simulation = 6 - Number of Epoch = 50\n",
      "Train loss:= 0.4424 - Val loss: 0.5132 - Test loss: 0.5497 - Train acc:= 84.38% - Val acc:= 81.71% - Test acc:= 81.41%\n",
      "Number of Simulation = 6 - Number of Epoch = 60\n",
      "Train loss:= 0.4146 - Val loss: 0.4929 - Test loss: 0.5289 - Train acc:= 85.33% - Val acc:= 82.49% - Test acc:= 82.12%\n",
      "Number of Simulation = 6 - Number of Epoch = 70\n",
      "Train loss:= 0.3915 - Val loss: 0.4765 - Test loss: 0.5124 - Train acc:= 86.09% - Val acc:= 83.10% - Test acc:= 82.56%\n",
      "Number of Simulation = 6 - Number of Epoch = 80\n",
      "Train loss:= 0.3718 - Val loss: 0.4629 - Test loss: 0.4984 - Train acc:= 86.80% - Val acc:= 83.64% - Test acc:= 83.02%\n",
      "Number of Simulation = 6 - Number of Epoch = 90\n",
      "Train loss:= 0.3546 - Val loss: 0.4513 - Test loss: 0.4863 - Train acc:= 87.34% - Val acc:= 84.07% - Test acc:= 83.55%\n",
      "Number of Simulation = 6 - Number of Epoch = 100\n",
      "Train loss:= 0.3392 - Val loss: 0.4409 - Test loss: 0.4757 - Train acc:= 87.87% - Val acc:= 84.44% - Test acc:= 83.77%\n",
      "Number of Simulation = 6 - Number of Epoch = 110\n",
      "Train loss:= 0.3254 - Val loss: 0.4316 - Test loss: 0.4664 - Train acc:= 88.41% - Val acc:= 84.84% - Test acc:= 83.97%\n",
      "Number of Simulation = 6 - Number of Epoch = 120\n",
      "Train loss:= 0.3128 - Val loss: 0.4230 - Test loss: 0.4580 - Train acc:= 88.89% - Val acc:= 85.00% - Test acc:= 84.23%\n",
      "Number of Simulation = 6 - Number of Epoch = 130\n",
      "Train loss:= 0.3013 - Val loss: 0.4151 - Test loss: 0.4504 - Train acc:= 89.34% - Val acc:= 85.15% - Test acc:= 84.58%\n",
      "Number of Simulation = 6 - Number of Epoch = 140\n",
      "Train loss:= 0.2907 - Val loss: 0.4078 - Test loss: 0.4434 - Train acc:= 89.77% - Val acc:= 85.43% - Test acc:= 84.84%\n",
      "Number of Simulation = 6 - Number of Epoch = 150\n",
      "Train loss:= 0.2808 - Val loss: 0.4010 - Test loss: 0.4369 - Train acc:= 90.17% - Val acc:= 85.80% - Test acc:= 85.01%\n",
      "Number of Simulation = 6 - Number of Epoch = 160\n",
      "Train loss:= 0.2716 - Val loss: 0.3948 - Test loss: 0.4308 - Train acc:= 90.48% - Val acc:= 85.98% - Test acc:= 85.30%\n",
      "Number of Simulation = 6 - Number of Epoch = 170\n",
      "Train loss:= 0.2630 - Val loss: 0.3890 - Test loss: 0.4251 - Train acc:= 90.82% - Val acc:= 85.99% - Test acc:= 85.54%\n",
      "Number of Simulation = 6 - Number of Epoch = 180\n",
      "Train loss:= 0.2549 - Val loss: 0.3836 - Test loss: 0.4198 - Train acc:= 91.11% - Val acc:= 86.16% - Test acc:= 85.63%\n",
      "Number of Simulation = 6 - Number of Epoch = 190\n",
      "Train loss:= 0.2473 - Val loss: 0.3785 - Test loss: 0.4147 - Train acc:= 91.34% - Val acc:= 86.39% - Test acc:= 85.89%\n",
      "Number of Simulation = 6 - Number of Epoch = 200\n",
      "Train loss:= 0.2401 - Val loss: 0.3737 - Test loss: 0.4099 - Train acc:= 91.63% - Val acc:= 86.63% - Test acc:= 86.12%\n",
      "\n",
      "Total time taken (in seconds): 1296.88\n",
      "Number of Simulation = 7 - Number of Epoch = 10\n",
      "Train loss:= 0.7293 - Val loss: 0.7737 - Test loss: 0.8176 - Train acc:= 76.26% - Val acc:= 74.78% - Test acc:= 73.75%\n",
      "Number of Simulation = 7 - Number of Epoch = 20\n",
      "Train loss:= 0.5886 - Val loss: 0.6476 - Test loss: 0.6906 - Train acc:= 80.06% - Val acc:= 78.06% - Test acc:= 77.05%\n",
      "Number of Simulation = 7 - Number of Epoch = 30\n",
      "Train loss:= 0.5202 - Val loss: 0.5900 - Test loss: 0.6318 - Train acc:= 82.04% - Val acc:= 79.82% - Test acc:= 78.81%\n",
      "Number of Simulation = 7 - Number of Epoch = 40\n",
      "Train loss:= 0.4736 - Val loss: 0.5518 - Test loss: 0.5934 - Train acc:= 83.46% - Val acc:= 81.08% - Test acc:= 79.85%\n",
      "Number of Simulation = 7 - Number of Epoch = 50\n",
      "Train loss:= 0.4375 - Val loss: 0.5232 - Test loss: 0.5648 - Train acc:= 84.74% - Val acc:= 81.79% - Test acc:= 80.79%\n",
      "Number of Simulation = 7 - Number of Epoch = 60\n",
      "Train loss:= 0.4090 - Val loss: 0.5014 - Test loss: 0.5427 - Train acc:= 85.64% - Val acc:= 82.70% - Test acc:= 81.50%\n",
      "Number of Simulation = 7 - Number of Epoch = 70\n",
      "Train loss:= 0.3855 - Val loss: 0.4839 - Test loss: 0.5248 - Train acc:= 86.39% - Val acc:= 83.32% - Test acc:= 82.07%\n",
      "Number of Simulation = 7 - Number of Epoch = 80\n",
      "Train loss:= 0.3654 - Val loss: 0.4694 - Test loss: 0.5100 - Train acc:= 87.05% - Val acc:= 83.91% - Test acc:= 82.51%\n",
      "Number of Simulation = 7 - Number of Epoch = 90\n",
      "Train loss:= 0.3479 - Val loss: 0.4571 - Test loss: 0.4975 - Train acc:= 87.71% - Val acc:= 84.21% - Test acc:= 82.84%\n",
      "Number of Simulation = 7 - Number of Epoch = 100\n",
      "Train loss:= 0.3325 - Val loss: 0.4463 - Test loss: 0.4864 - Train acc:= 88.23% - Val acc:= 84.51% - Test acc:= 83.16%\n",
      "Number of Simulation = 7 - Number of Epoch = 110\n",
      "Train loss:= 0.3187 - Val loss: 0.4369 - Test loss: 0.4766 - Train acc:= 88.72% - Val acc:= 84.85% - Test acc:= 83.53%\n",
      "Number of Simulation = 7 - Number of Epoch = 120\n",
      "Train loss:= 0.3062 - Val loss: 0.4285 - Test loss: 0.4676 - Train acc:= 89.21% - Val acc:= 85.15% - Test acc:= 83.77%\n",
      "Number of Simulation = 7 - Number of Epoch = 130\n",
      "Train loss:= 0.2949 - Val loss: 0.4209 - Test loss: 0.4593 - Train acc:= 89.59% - Val acc:= 85.46% - Test acc:= 84.01%\n",
      "Number of Simulation = 7 - Number of Epoch = 140\n",
      "Train loss:= 0.2845 - Val loss: 0.4140 - Test loss: 0.4514 - Train acc:= 89.99% - Val acc:= 85.68% - Test acc:= 84.24%\n",
      "Number of Simulation = 7 - Number of Epoch = 150\n",
      "Train loss:= 0.2750 - Val loss: 0.4077 - Test loss: 0.4440 - Train acc:= 90.32% - Val acc:= 85.99% - Test acc:= 84.50%\n",
      "Number of Simulation = 7 - Number of Epoch = 160\n",
      "Train loss:= 0.2663 - Val loss: 0.4020 - Test loss: 0.4372 - Train acc:= 90.59% - Val acc:= 86.26% - Test acc:= 84.70%\n",
      "Number of Simulation = 7 - Number of Epoch = 170\n",
      "Train loss:= 0.2582 - Val loss: 0.3967 - Test loss: 0.4307 - Train acc:= 90.92% - Val acc:= 86.45% - Test acc:= 84.91%\n",
      "Number of Simulation = 7 - Number of Epoch = 180\n",
      "Train loss:= 0.2506 - Val loss: 0.3919 - Test loss: 0.4246 - Train acc:= 91.24% - Val acc:= 86.55% - Test acc:= 85.17%\n",
      "Number of Simulation = 7 - Number of Epoch = 190\n",
      "Train loss:= 0.2435 - Val loss: 0.3876 - Test loss: 0.4190 - Train acc:= 91.51% - Val acc:= 86.63% - Test acc:= 85.38%\n",
      "Number of Simulation = 7 - Number of Epoch = 200\n",
      "Train loss:= 0.2367 - Val loss: 0.3835 - Test loss: 0.4137 - Train acc:= 91.77% - Val acc:= 86.76% - Test acc:= 85.56%\n",
      "\n",
      "Total time taken (in seconds): 1296.10\n",
      "Number of Simulation = 8 - Number of Epoch = 10\n",
      "Train loss:= 0.7073 - Val loss: 0.7490 - Test loss: 0.7785 - Train acc:= 76.27% - Val acc:= 74.54% - Test acc:= 74.12%\n",
      "Number of Simulation = 8 - Number of Epoch = 20\n",
      "Train loss:= 0.5755 - Val loss: 0.6295 - Test loss: 0.6634 - Train acc:= 80.15% - Val acc:= 78.25% - Test acc:= 77.70%\n",
      "Number of Simulation = 8 - Number of Epoch = 30\n",
      "Train loss:= 0.5070 - Val loss: 0.5756 - Test loss: 0.6116 - Train acc:= 82.26% - Val acc:= 79.83% - Test acc:= 79.56%\n",
      "Number of Simulation = 8 - Number of Epoch = 40\n",
      "Train loss:= 0.4619 - Val loss: 0.5407 - Test loss: 0.5774 - Train acc:= 83.77% - Val acc:= 81.17% - Test acc:= 80.63%\n",
      "Number of Simulation = 8 - Number of Epoch = 50\n",
      "Train loss:= 0.4286 - Val loss: 0.5148 - Test loss: 0.5518 - Train acc:= 84.93% - Val acc:= 81.91% - Test acc:= 81.35%\n",
      "Number of Simulation = 8 - Number of Epoch = 60\n",
      "Train loss:= 0.4023 - Val loss: 0.4943 - Test loss: 0.5315 - Train acc:= 85.80% - Val acc:= 82.62% - Test acc:= 82.03%\n",
      "Number of Simulation = 8 - Number of Epoch = 70\n",
      "Train loss:= 0.3804 - Val loss: 0.4774 - Test loss: 0.5149 - Train acc:= 86.51% - Val acc:= 83.17% - Test acc:= 82.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 8 - Number of Epoch = 80\n",
      "Train loss:= 0.3616 - Val loss: 0.4630 - Test loss: 0.5010 - Train acc:= 87.07% - Val acc:= 83.67% - Test acc:= 83.04%\n",
      "Number of Simulation = 8 - Number of Epoch = 90\n",
      "Train loss:= 0.3450 - Val loss: 0.4505 - Test loss: 0.4890 - Train acc:= 87.71% - Val acc:= 84.07% - Test acc:= 83.35%\n",
      "Number of Simulation = 8 - Number of Epoch = 100\n",
      "Train loss:= 0.3302 - Val loss: 0.4396 - Test loss: 0.4784 - Train acc:= 88.17% - Val acc:= 84.61% - Test acc:= 83.72%\n",
      "Number of Simulation = 8 - Number of Epoch = 110\n",
      "Train loss:= 0.3168 - Val loss: 0.4302 - Test loss: 0.4690 - Train acc:= 88.64% - Val acc:= 84.79% - Test acc:= 84.04%\n",
      "Number of Simulation = 8 - Number of Epoch = 120\n",
      "Train loss:= 0.3046 - Val loss: 0.4218 - Test loss: 0.4605 - Train acc:= 89.10% - Val acc:= 84.96% - Test acc:= 84.37%\n",
      "Number of Simulation = 8 - Number of Epoch = 130\n",
      "Train loss:= 0.2935 - Val loss: 0.4141 - Test loss: 0.4528 - Train acc:= 89.50% - Val acc:= 85.24% - Test acc:= 84.59%\n",
      "Number of Simulation = 8 - Number of Epoch = 140\n",
      "Train loss:= 0.2833 - Val loss: 0.4071 - Test loss: 0.4456 - Train acc:= 89.89% - Val acc:= 85.52% - Test acc:= 84.67%\n",
      "Number of Simulation = 8 - Number of Epoch = 150\n",
      "Train loss:= 0.2739 - Val loss: 0.4007 - Test loss: 0.4390 - Train acc:= 90.24% - Val acc:= 85.78% - Test acc:= 84.87%\n",
      "Number of Simulation = 8 - Number of Epoch = 160\n",
      "Train loss:= 0.2652 - Val loss: 0.3947 - Test loss: 0.4326 - Train acc:= 90.55% - Val acc:= 85.98% - Test acc:= 85.11%\n",
      "Number of Simulation = 8 - Number of Epoch = 170\n",
      "Train loss:= 0.2570 - Val loss: 0.3891 - Test loss: 0.4267 - Train acc:= 90.89% - Val acc:= 86.13% - Test acc:= 85.25%\n",
      "Number of Simulation = 8 - Number of Epoch = 180\n",
      "Train loss:= 0.2494 - Val loss: 0.3839 - Test loss: 0.4211 - Train acc:= 91.14% - Val acc:= 86.34% - Test acc:= 85.43%\n",
      "Number of Simulation = 8 - Number of Epoch = 190\n",
      "Train loss:= 0.2421 - Val loss: 0.3790 - Test loss: 0.4157 - Train acc:= 91.42% - Val acc:= 86.43% - Test acc:= 85.63%\n",
      "Number of Simulation = 8 - Number of Epoch = 200\n",
      "Train loss:= 0.2353 - Val loss: 0.3744 - Test loss: 0.4106 - Train acc:= 91.74% - Val acc:= 86.63% - Test acc:= 85.79%\n",
      "\n",
      "Total time taken (in seconds): 1308.04\n",
      "Number of Simulation = 9 - Number of Epoch = 10\n",
      "Train loss:= 0.7149 - Val loss: 0.7566 - Test loss: 0.7743 - Train acc:= 75.66% - Val acc:= 74.06% - Test acc:= 73.89%\n",
      "Number of Simulation = 9 - Number of Epoch = 20\n",
      "Train loss:= 0.5865 - Val loss: 0.6427 - Test loss: 0.6595 - Train acc:= 79.61% - Val acc:= 77.61% - Test acc:= 77.62%\n",
      "Number of Simulation = 9 - Number of Epoch = 30\n",
      "Train loss:= 0.5211 - Val loss: 0.5859 - Test loss: 0.6051 - Train acc:= 81.62% - Val acc:= 79.77% - Test acc:= 79.36%\n",
      "Number of Simulation = 9 - Number of Epoch = 40\n",
      "Train loss:= 0.4784 - Val loss: 0.5508 - Test loss: 0.5713 - Train acc:= 83.14% - Val acc:= 80.94% - Test acc:= 80.71%\n",
      "Number of Simulation = 9 - Number of Epoch = 50\n",
      "Train loss:= 0.4463 - Val loss: 0.5260 - Test loss: 0.5469 - Train acc:= 84.17% - Val acc:= 81.57% - Test acc:= 81.50%\n",
      "Number of Simulation = 9 - Number of Epoch = 60\n",
      "Train loss:= 0.4195 - Val loss: 0.5057 - Test loss: 0.5271 - Train acc:= 85.04% - Val acc:= 82.04% - Test acc:= 82.06%\n",
      "Number of Simulation = 9 - Number of Epoch = 70\n",
      "Train loss:= 0.3968 - Val loss: 0.4888 - Test loss: 0.5110 - Train acc:= 85.84% - Val acc:= 82.67% - Test acc:= 82.74%\n",
      "Number of Simulation = 9 - Number of Epoch = 80\n",
      "Train loss:= 0.3774 - Val loss: 0.4748 - Test loss: 0.4978 - Train acc:= 86.52% - Val acc:= 83.14% - Test acc:= 83.15%\n",
      "Number of Simulation = 9 - Number of Epoch = 90\n",
      "Train loss:= 0.3602 - Val loss: 0.4626 - Test loss: 0.4863 - Train acc:= 87.13% - Val acc:= 83.67% - Test acc:= 83.49%\n",
      "Number of Simulation = 9 - Number of Epoch = 100\n",
      "Train loss:= 0.3448 - Val loss: 0.4520 - Test loss: 0.4761 - Train acc:= 87.66% - Val acc:= 83.92% - Test acc:= 83.84%\n",
      "Number of Simulation = 9 - Number of Epoch = 110\n",
      "Train loss:= 0.3308 - Val loss: 0.4425 - Test loss: 0.4669 - Train acc:= 88.15% - Val acc:= 84.28% - Test acc:= 84.19%\n",
      "Number of Simulation = 9 - Number of Epoch = 120\n",
      "Train loss:= 0.3181 - Val loss: 0.4341 - Test loss: 0.4586 - Train acc:= 88.63% - Val acc:= 84.70% - Test acc:= 84.34%\n",
      "Number of Simulation = 9 - Number of Epoch = 130\n",
      "Train loss:= 0.3065 - Val loss: 0.4266 - Test loss: 0.4509 - Train acc:= 89.06% - Val acc:= 84.95% - Test acc:= 84.59%\n",
      "Number of Simulation = 9 - Number of Epoch = 140\n",
      "Train loss:= 0.2957 - Val loss: 0.4196 - Test loss: 0.4438 - Train acc:= 89.53% - Val acc:= 85.08% - Test acc:= 84.67%\n",
      "Number of Simulation = 9 - Number of Epoch = 150\n",
      "Train loss:= 0.2857 - Val loss: 0.4132 - Test loss: 0.4373 - Train acc:= 89.91% - Val acc:= 85.33% - Test acc:= 84.94%\n",
      "Number of Simulation = 9 - Number of Epoch = 160\n",
      "Train loss:= 0.2764 - Val loss: 0.4071 - Test loss: 0.4312 - Train acc:= 90.28% - Val acc:= 85.52% - Test acc:= 85.07%\n",
      "Number of Simulation = 9 - Number of Epoch = 170\n",
      "Train loss:= 0.2677 - Val loss: 0.4015 - Test loss: 0.4255 - Train acc:= 90.59% - Val acc:= 85.75% - Test acc:= 85.15%\n",
      "Number of Simulation = 9 - Number of Epoch = 180\n",
      "Train loss:= 0.2596 - Val loss: 0.3963 - Test loss: 0.4202 - Train acc:= 90.93% - Val acc:= 85.94% - Test acc:= 85.30%\n",
      "Number of Simulation = 9 - Number of Epoch = 190\n",
      "Train loss:= 0.2520 - Val loss: 0.3913 - Test loss: 0.4152 - Train acc:= 91.18% - Val acc:= 86.18% - Test acc:= 85.49%\n",
      "Number of Simulation = 9 - Number of Epoch = 200\n",
      "Train loss:= 0.2447 - Val loss: 0.3866 - Test loss: 0.4105 - Train acc:= 91.45% - Val acc:= 86.37% - Test acc:= 85.62%\n",
      "\n",
      "Total time taken (in seconds): 1310.25\n",
      "Number of Simulation = 10 - Number of Epoch = 10\n",
      "Train loss:= 0.7240 - Val loss: 0.7544 - Test loss: 0.7910 - Train acc:= 75.43% - Val acc:= 74.56% - Test acc:= 73.64%\n",
      "Number of Simulation = 10 - Number of Epoch = 20\n",
      "Train loss:= 0.5893 - Val loss: 0.6374 - Test loss: 0.6739 - Train acc:= 79.49% - Val acc:= 78.21% - Test acc:= 77.24%\n",
      "Number of Simulation = 10 - Number of Epoch = 30\n",
      "Train loss:= 0.5216 - Val loss: 0.5855 - Test loss: 0.6207 - Train acc:= 81.62% - Val acc:= 79.70% - Test acc:= 79.10%\n",
      "Number of Simulation = 10 - Number of Epoch = 40\n",
      "Train loss:= 0.4762 - Val loss: 0.5530 - Test loss: 0.5868 - Train acc:= 83.06% - Val acc:= 80.64% - Test acc:= 80.10%\n",
      "Number of Simulation = 10 - Number of Epoch = 50\n",
      "Train loss:= 0.4422 - Val loss: 0.5292 - Test loss: 0.5616 - Train acc:= 84.20% - Val acc:= 81.46% - Test acc:= 80.84%\n",
      "Number of Simulation = 10 - Number of Epoch = 60\n",
      "Train loss:= 0.4149 - Val loss: 0.5103 - Test loss: 0.5413 - Train acc:= 85.18% - Val acc:= 82.08% - Test acc:= 81.36%\n",
      "Number of Simulation = 10 - Number of Epoch = 70\n",
      "Train loss:= 0.3922 - Val loss: 0.4945 - Test loss: 0.5245 - Train acc:= 85.92% - Val acc:= 82.53% - Test acc:= 81.98%\n",
      "Number of Simulation = 10 - Number of Epoch = 80\n",
      "Train loss:= 0.3728 - Val loss: 0.4812 - Test loss: 0.5103 - Train acc:= 86.58% - Val acc:= 82.99% - Test acc:= 82.51%\n",
      "Number of Simulation = 10 - Number of Epoch = 90\n",
      "Train loss:= 0.3560 - Val loss: 0.4695 - Test loss: 0.4980 - Train acc:= 87.17% - Val acc:= 83.37% - Test acc:= 82.96%\n",
      "Number of Simulation = 10 - Number of Epoch = 100\n",
      "Train loss:= 0.3412 - Val loss: 0.4590 - Test loss: 0.4870 - Train acc:= 87.70% - Val acc:= 83.70% - Test acc:= 83.16%\n",
      "Number of Simulation = 10 - Number of Epoch = 110\n",
      "Train loss:= 0.3280 - Val loss: 0.4494 - Test loss: 0.4769 - Train acc:= 88.18% - Val acc:= 84.05% - Test acc:= 83.52%\n",
      "Number of Simulation = 10 - Number of Epoch = 120\n",
      "Train loss:= 0.3159 - Val loss: 0.4405 - Test loss: 0.4675 - Train acc:= 88.65% - Val acc:= 84.37% - Test acc:= 83.74%\n",
      "Number of Simulation = 10 - Number of Epoch = 130\n",
      "Train loss:= 0.3049 - Val loss: 0.4322 - Test loss: 0.4587 - Train acc:= 89.06% - Val acc:= 84.67% - Test acc:= 84.04%\n",
      "Number of Simulation = 10 - Number of Epoch = 140\n",
      "Train loss:= 0.2946 - Val loss: 0.4246 - Test loss: 0.4507 - Train acc:= 89.44% - Val acc:= 84.85% - Test acc:= 84.25%\n",
      "Number of Simulation = 10 - Number of Epoch = 150\n",
      "Train loss:= 0.2850 - Val loss: 0.4175 - Test loss: 0.4434 - Train acc:= 89.82% - Val acc:= 85.06% - Test acc:= 84.41%\n",
      "Number of Simulation = 10 - Number of Epoch = 160\n",
      "Train loss:= 0.2760 - Val loss: 0.4109 - Test loss: 0.4366 - Train acc:= 90.14% - Val acc:= 85.25% - Test acc:= 84.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 10 - Number of Epoch = 170\n",
      "Train loss:= 0.2675 - Val loss: 0.4047 - Test loss: 0.4302 - Train acc:= 90.44% - Val acc:= 85.52% - Test acc:= 84.77%\n",
      "Number of Simulation = 10 - Number of Epoch = 180\n",
      "Train loss:= 0.2595 - Val loss: 0.3989 - Test loss: 0.4242 - Train acc:= 90.77% - Val acc:= 85.70% - Test acc:= 84.94%\n",
      "Number of Simulation = 10 - Number of Epoch = 190\n",
      "Train loss:= 0.2518 - Val loss: 0.3935 - Test loss: 0.4185 - Train acc:= 91.08% - Val acc:= 86.04% - Test acc:= 85.14%\n",
      "Number of Simulation = 10 - Number of Epoch = 200\n",
      "Train loss:= 0.2445 - Val loss: 0.3885 - Test loss: 0.4132 - Train acc:= 91.32% - Val acc:= 86.35% - Test acc:= 85.45%\n",
      "\n",
      "Total time taken (in seconds): 1326.20\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "    #mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1.0, 'lr':1e-4}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs)\n",
    "\n",
    "            #use adam to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams)\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final average result: test loss := 0.4110 - test acc := 85.71% - the std of test acc := 0.0293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_average_loss = np.mean(train_loss_record,0)\n",
    "train_average_acc = np.mean(train_acc_record,0)\n",
    "val_average_loss = np.mean(val_loss_record,0)\n",
    "val_average_acc = np.mean(val_acc_record,0)\n",
    "test_average_loss = np.mean(test_loss_record,0)\n",
    "test_average_acc = np.mean(test_acc_record,0)\n",
    "\n",
    "test_var_acc = np.var(test_acc_record,1)[-1]\n",
    "x = np.linspace(1,NUM_EPOCHS,len(train_average_loss))\n",
    "\n",
    "plt.figure()\n",
    "f, ax = plt.subplots(1,2, sharex = True, sharey = False,figsize=(8,3))\n",
    "f.suptitle('Report of MLP with l2 regularizer for MNIST fashion data set')\n",
    "\n",
    "\n",
    "ax[0].plot(x,train_average_loss,label='train')\n",
    "ax[0].plot(x,val_average_loss,label='val')\n",
    "ax[0].plot(x,test_average_loss,label='test')\n",
    "#ax[0].set_yscale('log')\n",
    "ax[0].set_title('')\n",
    "ax.flat[0].set(xlabel='Num of epochs', ylabel='Average loss')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(x,train_average_acc,label='train')\n",
    "ax[1].plot(x,val_average_acc,label='val')\n",
    "ax[1].plot(x,test_average_acc,label='test')\n",
    "ax[1].set_title('')\n",
    "ax.flat[1].set(xlabel='Num of epochs', ylabel='Average accuray')\n",
    "ax[1].legend()\n",
    "\n",
    "print('the final average result: test loss := {:.4f} - test acc := {:.2%} - the std of test acc := {:.4f}'\n",
    "      .format(test_average_loss[-1], test_average_acc[-1], np.sqrt(test_var_acc)))\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
