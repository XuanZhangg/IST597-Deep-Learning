{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "391\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_val = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n",
    "x_train = x_train.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_val = x_val.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_test = x_test.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "train_dataset_full = train_dataset.shuffle(buffer_size=1024).batch(len(train_dataset))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecognitionCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):\n",
    "        ''' Define the parameterized layers used during forward-pass, the device\n",
    "            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint\n",
    "            directory.\n",
    "            \n",
    "            Args:\n",
    "                num_classes: the number of labels in the network.\n",
    "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
    "                checkpoint_directory: the directory where you would like to save or \n",
    "                                      restore a model.\n",
    "        ''' \n",
    "        super(ImageRecognitionCNN, self).__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3,padding='same', activation=None)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv5 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        \n",
    "        # Define the device \n",
    "        self.device = device\n",
    "        \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def predict(self, images, training):\n",
    "        \"\"\" Predicts the probability of each class, based on the input sample.\n",
    "            \n",
    "            Args:\n",
    "                images: 4D tensor. Either an image or a batch of images.\n",
    "                training: Boolean. Either the network is predicting in\n",
    "                          training mode or not.\n",
    "        \"\"\"\n",
    "        x = self.conv1(images)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv5(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = tf.reshape(x, (-1, 1, 10))\n",
    "        #x = tf.keras.layers.Flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def loss_fn(self, images, target, training):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        preds = self.predict(images, training)\n",
    "        #print(preds.shape)\n",
    "        #print(target.shape)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def grads_fn(self, images, target, training):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(images, target, training)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            # Run the model once to initialize variables\n",
    "            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\n",
    "            dummy_pred = self.predict(dummy_input, training=False)\n",
    "            # Restore the variables of the model\n",
    "            saver = tf.Saver(self.variables)\n",
    "            saver.restore(tf.train.latest_checkpoint\n",
    "                          (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tf.Saver(self.variables).save(self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "    \n",
    "    # def compute_accuracy(self, input_data):\n",
    "    #     \"\"\" Compute the accuracy on the input data.\n",
    "    #     \"\"\"\n",
    "    #     with tf.device(self.device):\n",
    "    #         #acc = tf.metrics.Accuracy()\n",
    "    #         for step ,(images, targets) in enumerate(input_data):\n",
    "    #             # Predict the probability of each class\n",
    "    #             #print(targets.shape)\n",
    "    #             logits = self.predict(images, training=False)\n",
    "    #             # Select the class with the highest probability\n",
    "    #             #print(logits.shape)\n",
    "    #             logits = tf.nn.softmax(logits)\n",
    "    #             logits = tf.reshape(logits, [-1, 10])\n",
    "    #             targets = tf.reshape(targets, [-1,10])\n",
    "    #             preds = tf.argmax(logits, axis=1)\n",
    "                \n",
    "    #             #m1.update_state\n",
    "    #             # Compute the accuracy\n",
    "    #             #print(preds.shape)\n",
    "    #             acc(tf.reshape(targets, preds))\n",
    "    #     return acc\n",
    "\n",
    "    def compute_accuracy_2(self, images, targets):\n",
    "        \"\"\" Compute the accuracy on the input data.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            \n",
    "            # Predict the probability of each class\n",
    "            logits = self.predict(images, training=False)\n",
    "            # Select the class with the highest probability\n",
    "            \n",
    "            logits = tf.nn.softmax(logits)\n",
    "            logits = tf.reshape(logits, [-1, 10])\n",
    "            targets = tf.reshape(targets, [-1,10])\n",
    "            preds = tf.argmax(logits, axis=1)\n",
    "            goal = tf.argmax(targets, axis=1)\n",
    "            self.acc.update_state(goal, preds)\n",
    "            # Compute the accuracy\n",
    "            result = self.acc.result().numpy()\n",
    "        return result\n",
    "\n",
    "  \n",
    "    def fit_fc(self, training_data, eval_data, test_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the loss on the eval \n",
    "                                       dataset does not decrease after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best loss. This variable will store the lowest loss on the\n",
    "        # eval dataset.\n",
    "        best_loss = 999\n",
    "        \n",
    "        # Initialize classes to update the mean loss of train and eval\n",
    "        train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "        eval_loss = tf.keras.metrics.Mean('eval_loss')\n",
    "        test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "        acc_train = tf.keras.metrics.Mean('train_acc')\n",
    "        acc_val = tf.keras.metrics.Mean('val_acc')\n",
    "        acc_test = tf.keras.metrics.Mean('test_acc')\n",
    "        \n",
    "        # Initialize dictionary to store the loss history\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['eval_loss'] = []\n",
    "        self.history['test_loss'] = []\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['val_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        with tf.device(self.device):\n",
    "            for i in range(num_epochs):\n",
    "                # Training with gradient descent\n",
    "                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    grads = self.grads_fn(images, target, True)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                # Compute the loss on the training data after one epoch\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_train(accuracy)\n",
    "                    train_loss(loss)\n",
    "                self.history['train_loss'].append(train_loss.result().numpy())\n",
    "                self.history['train_acc'].append(acc_train.result().numpy())\n",
    "                # Reset metrics\n",
    "                train_loss.reset_states()\n",
    "                acc_train.reset_states()\n",
    "                \n",
    "                # Compute the loss on the eval data after one epoch\n",
    "                for step, (images, target) in enumerate(eval_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['eval_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['val_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Compute the loss on the test data after one epoch\n",
    "                for step, (images, target) in enumerate(test_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['test_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['test_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Print train and eval losses\n",
    "                if (i==0) | ((i+1)%verbose==0):\n",
    "                    print('Number of Epoch = {}'.format(i + 1))\n",
    "                    print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(self.history['train_loss'][-1], self.history['eval_loss'][-1], self.history['test_loss'][-1], \\\n",
    "                          self.history['train_acc'][-1], self.history['val_acc'][-1], self.history['test_acc'][-1]))\n",
    "\n",
    "\n",
    "                # Check for early stopping\n",
    "                if self.history['eval_loss'][-1]<best_loss:\n",
    "                    best_loss = self.history['eval_loss'][-1]\n",
    "                    count = early_stopping_rounds\n",
    "                else:\n",
    "                    count -= 1\n",
    "                if count==0:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1\n",
      "Train loss:= 0.6531 - Val loss: 0.6487 - Test loss: 0.6733 - Train acc:= 77.05% - Val acc:= 76.94% - Test acc:= 76.85%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3534 - Val loss: 0.3745 - Test loss: 0.3921 - Train acc:= 84.20% - Val acc:= 84.33% - Test acc:= 84.36%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2660 - Val loss: 0.2937 - Test loss: 0.3096 - Train acc:= 86.46% - Val acc:= 86.55% - Test acc:= 86.56%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2553 - Val loss: 0.2879 - Test loss: 0.3076 - Train acc:= 87.78% - Val acc:= 87.82% - Test acc:= 87.83%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2122 - Val loss: 0.2609 - Test loss: 0.2774 - Train acc:= 88.72% - Val acc:= 88.75% - Test acc:= 88.76%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1829 - Val loss: 0.2440 - Test loss: 0.2616 - Train acc:= 89.44% - Val acc:= 89.47% - Test acc:= 89.47%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1700 - Val loss: 0.2477 - Test loss: 0.2658 - Train acc:= 90.01% - Val acc:= 90.04% - Test acc:= 90.04%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1520 - Val loss: 0.2466 - Test loss: 0.2660 - Train acc:= 90.49% - Val acc:= 90.51% - Test acc:= 90.52%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1276 - Val loss: 0.2412 - Test loss: 0.2611 - Train acc:= 90.89% - Val acc:= 90.91% - Test acc:= 90.91%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1318 - Val loss: 0.2628 - Test loss: 0.2841 - Train acc:= 91.27% - Val acc:= 91.29% - Test acc:= 91.29%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1061 - Val loss: 0.2587 - Test loss: 0.2839 - Train acc:= 91.62% - Val acc:= 91.63% - Test acc:= 91.63%\n",
      "Number of Epoch = 1\n",
      "Train loss:= 0.6908 - Val loss: 0.6804 - Test loss: 0.7124 - Train acc:= 75.09% - Val acc:= 75.09% - Test acc:= 75.07%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3521 - Val loss: 0.3722 - Test loss: 0.3888 - Train acc:= 83.52% - Val acc:= 83.69% - Test acc:= 83.73%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2760 - Val loss: 0.3036 - Test loss: 0.3186 - Train acc:= 86.01% - Val acc:= 86.09% - Test acc:= 86.11%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2722 - Val loss: 0.3033 - Test loss: 0.3224 - Train acc:= 87.39% - Val acc:= 87.43% - Test acc:= 87.43%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2135 - Val loss: 0.2592 - Test loss: 0.2785 - Train acc:= 88.35% - Val acc:= 88.39% - Test acc:= 88.40%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1895 - Val loss: 0.2456 - Test loss: 0.2639 - Train acc:= 89.06% - Val acc:= 89.10% - Test acc:= 89.10%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1695 - Val loss: 0.2402 - Test loss: 0.2569 - Train acc:= 89.66% - Val acc:= 89.69% - Test acc:= 89.69%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1540 - Val loss: 0.2371 - Test loss: 0.2578 - Train acc:= 90.16% - Val acc:= 90.18% - Test acc:= 90.18%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1419 - Val loss: 0.2418 - Test loss: 0.2616 - Train acc:= 90.60% - Val acc:= 90.62% - Test acc:= 90.63%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1373 - Val loss: 0.2603 - Test loss: 0.2725 - Train acc:= 91.00% - Val acc:= 91.02% - Test acc:= 91.02%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1084 - Val loss: 0.2525 - Test loss: 0.2687 - Train acc:= 91.35% - Val acc:= 91.37% - Test acc:= 91.37%\n",
      "Number of Epoch = 1\n",
      "Train loss:= 0.6487 - Val loss: 0.6477 - Test loss: 0.6694 - Train acc:= 75.80% - Val acc:= 75.70% - Test acc:= 75.61%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3274 - Val loss: 0.3479 - Test loss: 0.3667 - Train acc:= 84.21% - Val acc:= 84.39% - Test acc:= 84.43%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2695 - Val loss: 0.2992 - Test loss: 0.3168 - Train acc:= 86.69% - Val acc:= 86.77% - Test acc:= 86.79%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2320 - Val loss: 0.2750 - Test loss: 0.2924 - Train acc:= 87.93% - Val acc:= 87.98% - Test acc:= 87.99%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2050 - Val loss: 0.2631 - Test loss: 0.2778 - Train acc:= 88.79% - Val acc:= 88.83% - Test acc:= 88.83%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1960 - Val loss: 0.2648 - Test loss: 0.2818 - Train acc:= 89.47% - Val acc:= 89.49% - Test acc:= 89.49%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1613 - Val loss: 0.2521 - Test loss: 0.2665 - Train acc:= 90.01% - Val acc:= 90.04% - Test acc:= 90.04%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1612 - Val loss: 0.2733 - Test loss: 0.2854 - Train acc:= 90.46% - Val acc:= 90.48% - Test acc:= 90.48%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1491 - Val loss: 0.2786 - Test loss: 0.2977 - Train acc:= 90.86% - Val acc:= 90.87% - Test acc:= 90.87%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1189 - Val loss: 0.2698 - Test loss: 0.2858 - Train acc:= 91.20% - Val acc:= 91.22% - Test acc:= 91.22%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1006 - Val loss: 0.2816 - Test loss: 0.2917 - Train acc:= 91.53% - Val acc:= 91.55% - Test acc:= 91.55%\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/mnist/'\n",
    "\n",
    "# Use the GPU if available.\n",
    "device = 'gpu:0'\n",
    "\n",
    "for i in range(3):\n",
    "    np.random.seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    # Define optimizer.\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "    # Instantiate model. This doesn't initialize the variables yet.\n",
    "    model = ImageRecognitionCNN(num_classes=10, device=device, \n",
    "                                  checkpoint_directory=checkpoint_directory)\n",
    "\n",
    "    #model = ImageRecognitionCNN(num_classes=7, device=device)\n",
    "    # Train model\n",
    "    model.fit_fc(train_dataset, val_dataset, test_dataset, optimizer, num_epochs=100, \n",
    "              early_stopping_rounds=100, verbose=10, train_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
