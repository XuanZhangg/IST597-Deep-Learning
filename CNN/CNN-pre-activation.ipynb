{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "391\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_val = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n",
    "x_train = x_train.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_val = x_val.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_test = x_test.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "train_dataset_full = train_dataset.shuffle(buffer_size=1024).batch(len(train_dataset))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecognitionCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):\n",
    "        ''' Define the parameterized layers used during forward-pass, the device\n",
    "            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint\n",
    "            directory.\n",
    "            \n",
    "            Args:\n",
    "                num_classes: the number of labels in the network.\n",
    "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
    "                checkpoint_directory: the directory where you would like to save or \n",
    "                                      restore a model.\n",
    "        ''' \n",
    "        super(ImageRecognitionCNN, self).__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3,padding='same', activation=None)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv5 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        \n",
    "        # Define the device \n",
    "        self.device = device\n",
    "        \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def predict(self, images, training):\n",
    "        \"\"\" Predicts the probability of each class, based on the input sample.\n",
    "            \n",
    "            Args:\n",
    "                images: 4D tensor. Either an image or a batch of images.\n",
    "                training: Boolean. Either the network is predicting in\n",
    "                          training mode or not.\n",
    "        \"\"\"\n",
    "        x = self.conv1(images)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv5(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = tf.reshape(x, (-1, 1, 10))\n",
    "        #x = tf.keras.layers.Flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def loss_fn(self, images, target, training):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        preds = self.predict(images, training)\n",
    "        #print(preds.shape)\n",
    "        #print(target.shape)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def grads_fn(self, images, target, training):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(images, target, training)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            # Run the model once to initialize variables\n",
    "            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\n",
    "            dummy_pred = self.predict(dummy_input, training=False)\n",
    "            # Restore the variables of the model\n",
    "            saver = tf.Saver(self.variables)\n",
    "            saver.restore(tf.train.latest_checkpoint\n",
    "                          (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tf.Saver(self.variables).save(self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "    \n",
    "    # def compute_accuracy(self, input_data):\n",
    "    #     \"\"\" Compute the accuracy on the input data.\n",
    "    #     \"\"\"\n",
    "    #     with tf.device(self.device):\n",
    "    #         #acc = tf.metrics.Accuracy()\n",
    "    #         for step ,(images, targets) in enumerate(input_data):\n",
    "    #             # Predict the probability of each class\n",
    "    #             #print(targets.shape)\n",
    "    #             logits = self.predict(images, training=False)\n",
    "    #             # Select the class with the highest probability\n",
    "    #             #print(logits.shape)\n",
    "    #             logits = tf.nn.softmax(logits)\n",
    "    #             logits = tf.reshape(logits, [-1, 10])\n",
    "    #             targets = tf.reshape(targets, [-1,10])\n",
    "    #             preds = tf.argmax(logits, axis=1)\n",
    "                \n",
    "    #             #m1.update_state\n",
    "    #             # Compute the accuracy\n",
    "    #             #print(preds.shape)\n",
    "    #             acc(tf.reshape(targets, preds))\n",
    "    #     return acc\n",
    "\n",
    "    def compute_accuracy_2(self, images, targets):\n",
    "        \"\"\" Compute the accuracy on the input data.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            \n",
    "            # Predict the probability of each class\n",
    "            logits = self.predict(images, training=False)\n",
    "            # Select the class with the highest probability\n",
    "            \n",
    "            logits = tf.nn.softmax(logits)\n",
    "            logits = tf.reshape(logits, [-1, 10])\n",
    "            targets = tf.reshape(targets, [-1,10])\n",
    "            preds = tf.argmax(logits, axis=1)\n",
    "            goal = tf.argmax(targets, axis=1)\n",
    "            self.acc.update_state(goal, preds)\n",
    "            # Compute the accuracy\n",
    "            result = self.acc.result().numpy()\n",
    "        return result\n",
    "\n",
    "  \n",
    "    def fit_fc(self, training_data, eval_data, test_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the loss on the eval \n",
    "                                       dataset does not decrease after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best loss. This variable will store the lowest loss on the\n",
    "        # eval dataset.\n",
    "        best_loss = 999\n",
    "        \n",
    "        # Initialize classes to update the mean loss of train and eval\n",
    "        train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "        eval_loss = tf.keras.metrics.Mean('eval_loss')\n",
    "        test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "        acc_train = tf.keras.metrics.Mean('train_acc')\n",
    "        acc_val = tf.keras.metrics.Mean('val_acc')\n",
    "        acc_test = tf.keras.metrics.Mean('test_acc')\n",
    "        \n",
    "        # Initialize dictionary to store the loss history\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['eval_loss'] = []\n",
    "        self.history['test_loss'] = []\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['val_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        with tf.device(self.device):\n",
    "            for i in range(num_epochs):\n",
    "                # Training with gradient descent\n",
    "                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    grads = self.grads_fn(images, target, True)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                # Compute the loss on the training data after one epoch\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_train(accuracy)\n",
    "                    train_loss(loss)\n",
    "                self.history['train_loss'].append(train_loss.result().numpy())\n",
    "                self.history['train_acc'].append(acc_train.result().numpy())\n",
    "                # Reset metrics\n",
    "                train_loss.reset_states()\n",
    "                acc_train.reset_states()\n",
    "                \n",
    "                # Compute the loss on the eval data after one epoch\n",
    "                for step, (images, target) in enumerate(eval_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['eval_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['val_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Compute the loss on the test data after one epoch\n",
    "                for step, (images, target) in enumerate(test_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['test_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['test_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Print train and eval losses\n",
    "                if (i==0) | ((i+1)%verbose==0):\n",
    "                    print('Number of Epoch = {}'.format(i + 1))\n",
    "                    print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(self.history['train_loss'][-1], self.history['eval_loss'][-1], self.history['test_loss'][-1], \\\n",
    "                          self.history['train_acc'][-1], self.history['val_acc'][-1], self.history['test_acc'][-1]))\n",
    "\n",
    "\n",
    "                # Check for early stopping\n",
    "                if self.history['eval_loss'][-1]<best_loss:\n",
    "                    best_loss = self.history['eval_loss'][-1]\n",
    "                    count = early_stopping_rounds\n",
    "                else:\n",
    "                    count -= 1\n",
    "                if count==0:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1\n",
      "Train loss:= 0.6492 - Val loss: 0.6445 - Test loss: 0.6701 - Train acc:= 76.64% - Val acc:= 76.65% - Test acc:= 76.58%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3304 - Val loss: 0.3475 - Test loss: 0.3662 - Train acc:= 84.21% - Val acc:= 84.38% - Test acc:= 84.43%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2687 - Val loss: 0.2963 - Test loss: 0.3114 - Train acc:= 86.51% - Val acc:= 86.60% - Test acc:= 86.61%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2349 - Val loss: 0.2717 - Test loss: 0.2890 - Train acc:= 87.85% - Val acc:= 87.90% - Test acc:= 87.91%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2152 - Val loss: 0.2621 - Test loss: 0.2806 - Train acc:= 88.76% - Val acc:= 88.80% - Test acc:= 88.80%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1883 - Val loss: 0.2450 - Test loss: 0.2664 - Train acc:= 89.48% - Val acc:= 89.51% - Test acc:= 89.51%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1659 - Val loss: 0.2399 - Test loss: 0.2628 - Train acc:= 90.05% - Val acc:= 90.07% - Test acc:= 90.07%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1637 - Val loss: 0.2553 - Test loss: 0.2792 - Train acc:= 90.51% - Val acc:= 90.53% - Test acc:= 90.53%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1364 - Val loss: 0.2511 - Test loss: 0.2740 - Train acc:= 90.94% - Val acc:= 90.96% - Test acc:= 90.96%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1193 - Val loss: 0.2529 - Test loss: 0.2783 - Train acc:= 91.31% - Val acc:= 91.33% - Test acc:= 91.33%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1046 - Val loss: 0.2647 - Test loss: 0.2876 - Train acc:= 91.65% - Val acc:= 91.67% - Test acc:= 91.67%\n",
      "Number of Epoch = 1\n",
      "Train loss:= 0.6905 - Val loss: 0.6840 - Test loss: 0.7120 - Train acc:= 74.49% - Val acc:= 74.37% - Test acc:= 74.32%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3394 - Val loss: 0.3577 - Test loss: 0.3770 - Train acc:= 83.38% - Val acc:= 83.58% - Test acc:= 83.63%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2866 - Val loss: 0.3115 - Test loss: 0.3308 - Train acc:= 86.12% - Val acc:= 86.19% - Test acc:= 86.21%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2331 - Val loss: 0.2686 - Test loss: 0.2876 - Train acc:= 87.52% - Val acc:= 87.58% - Test acc:= 87.59%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2153 - Val loss: 0.2610 - Test loss: 0.2808 - Train acc:= 88.51% - Val acc:= 88.55% - Test acc:= 88.55%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1874 - Val loss: 0.2468 - Test loss: 0.2673 - Train acc:= 89.22% - Val acc:= 89.25% - Test acc:= 89.25%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1704 - Val loss: 0.2418 - Test loss: 0.2658 - Train acc:= 89.79% - Val acc:= 89.81% - Test acc:= 89.82%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1533 - Val loss: 0.2401 - Test loss: 0.2616 - Train acc:= 90.29% - Val acc:= 90.31% - Test acc:= 90.32%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1304 - Val loss: 0.2342 - Test loss: 0.2568 - Train acc:= 90.72% - Val acc:= 90.74% - Test acc:= 90.74%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1157 - Val loss: 0.2413 - Test loss: 0.2637 - Train acc:= 91.11% - Val acc:= 91.13% - Test acc:= 91.13%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1022 - Val loss: 0.2491 - Test loss: 0.2709 - Train acc:= 91.44% - Val acc:= 91.46% - Test acc:= 91.46%\n",
      "Number of Epoch = 1\n",
      "Train loss:= 0.6372 - Val loss: 0.6375 - Test loss: 0.6579 - Train acc:= 76.70% - Val acc:= 76.60% - Test acc:= 76.49%\n",
      "Number of Epoch = 10\n",
      "Train loss:= 0.3266 - Val loss: 0.3432 - Test loss: 0.3647 - Train acc:= 84.50% - Val acc:= 84.66% - Test acc:= 84.70%\n",
      "Number of Epoch = 20\n",
      "Train loss:= 0.2727 - Val loss: 0.2987 - Test loss: 0.3219 - Train acc:= 86.72% - Val acc:= 86.79% - Test acc:= 86.81%\n",
      "Number of Epoch = 30\n",
      "Train loss:= 0.2322 - Val loss: 0.2708 - Test loss: 0.2941 - Train acc:= 87.94% - Val acc:= 87.99% - Test acc:= 88.00%\n",
      "Number of Epoch = 40\n",
      "Train loss:= 0.2013 - Val loss: 0.2519 - Test loss: 0.2752 - Train acc:= 88.81% - Val acc:= 88.85% - Test acc:= 88.86%\n",
      "Number of Epoch = 50\n",
      "Train loss:= 0.1798 - Val loss: 0.2435 - Test loss: 0.2699 - Train acc:= 89.46% - Val acc:= 89.49% - Test acc:= 89.49%\n",
      "Number of Epoch = 60\n",
      "Train loss:= 0.1623 - Val loss: 0.2403 - Test loss: 0.2714 - Train acc:= 90.02% - Val acc:= 90.04% - Test acc:= 90.05%\n",
      "Number of Epoch = 70\n",
      "Train loss:= 0.1495 - Val loss: 0.2468 - Test loss: 0.2765 - Train acc:= 90.48% - Val acc:= 90.51% - Test acc:= 90.51%\n",
      "Number of Epoch = 80\n",
      "Train loss:= 0.1493 - Val loss: 0.2572 - Test loss: 0.2935 - Train acc:= 90.88% - Val acc:= 90.90% - Test acc:= 90.90%\n",
      "Number of Epoch = 90\n",
      "Train loss:= 0.1153 - Val loss: 0.2455 - Test loss: 0.2834 - Train acc:= 91.23% - Val acc:= 91.25% - Test acc:= 91.25%\n",
      "Number of Epoch = 100\n",
      "Train loss:= 0.1075 - Val loss: 0.2584 - Test loss: 0.2994 - Train acc:= 91.54% - Val acc:= 91.56% - Test acc:= 91.56%\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/mnist/'\n",
    "\n",
    "# Use the GPU if available.\n",
    "device = 'gpu:0'\n",
    "\n",
    "for i in range(3):\n",
    "    np.random.seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    # Define optimizer.\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "    # Instantiate model. This doesn't initialize the variables yet.\n",
    "    model = ImageRecognitionCNN(num_classes=10, device=device, \n",
    "                                  checkpoint_directory=checkpoint_directory)\n",
    "\n",
    "    #model = ImageRecognitionCNN(num_classes=7, device=device)\n",
    "    # Train model\n",
    "    model.fit_fc(train_dataset, val_dataset, test_dataset, optimizer, num_epochs=100, \n",
    "              early_stopping_rounds=100, verbose=10, train_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
