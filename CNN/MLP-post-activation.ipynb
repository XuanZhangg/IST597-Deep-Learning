{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: (50000, 784)\n",
      "val_size: (10000, 784)\n",
      "test_size: (10000, 784)\n",
      "train_output_size (50000,)\n",
      "val_output_size: (10000,)\n",
      "test_output_size (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load and normalize data\n",
    "#(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = tf.reshape(X_train, (X_train.shape[0],-1))/255\n",
    "X_test = tf.reshape(X_test, (X_test.shape[0],-1))/255\n",
    "\n",
    "#reserve the last 10000 training examples for validation\n",
    "X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "print(\"train_size:\", X_train.shape)\n",
    "print(\"val_size:\", X_val.shape)\n",
    "print(\"test_size:\", X_test.shape)\n",
    "print(\"train_output_size\", y_train.shape)\n",
    "print(\"val_output_size:\", y_val.shape)\n",
    "print(\"test_output_size\", y_test.shape)\n",
    "\n",
    "size_input = X_train.shape[1]\n",
    "size_output = len(set(y_train))\n",
    "size_hidden1 = 128\n",
    "size_hidden2 = 128\n",
    "size_hidden3 = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with BN -- BN(g(f(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        regularizer: str or None\n",
    "        R_lambda: the parameter for regularizer\n",
    "        drop_prob: 0 to 1\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "        \n",
    "        \n",
    "        # Initialize BN scale parameter in the input layer\n",
    "        self.gamma0 = tf.Variable(tf.ones([1, self.size_input]))\n",
    "        # Initialize BN shift parameter in the input layer\n",
    "        self.beta0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        # The moving mean and variance in the input layer\n",
    "        self.moving_mean0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        self.moving_var0 = tf.Variable(tf.ones([1, self.size_input]))\n",
    "        \n",
    "        # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 1\n",
    "        self.gamma1 = tf.Variable(tf.ones([1, self.size_hidden1]))\n",
    "        #Initialize BN shift parameter in the hidden layer 1\n",
    "        self.beta1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        # The moving mean and variance in the input layer\n",
    "        self.moving_mean1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        self.moving_var1 = tf.Variable(tf.ones([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights between input layer and 1st hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 2\n",
    "        self.gamma2 = tf.Variable(tf.ones([1, self.size_hidden2]))\n",
    "        # Initialize BN shift parameter in the hidden layer 2\n",
    "        self.beta2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        # The moving mean and variance in the input layer\n",
    "        self.moving_mean2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        self.moving_var2 = tf.Variable(tf.ones([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 3\n",
    "        self.gamma3 = tf.Variable(tf.ones([1, self.size_hidden3]))\n",
    "        #Initialize BN shift parameter in the hidden layer 3\n",
    "        self.beta3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        # The moving mean and variance in the input layer\n",
    "        self.moving_mean3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        self.moving_var3 = tf.Variable(tf.ones([1, self.size_hidden3]))\n",
    "\n",
    "         # Initialize weights between 2nd hidden layer and output layer\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.W4, \\\n",
    "                          self.b1, self.b2, self.b3, self.b4, \\\n",
    "                          self.gamma0, self.gamma1, self.gamma2, self.gamma3, \\\n",
    "                          self.beta0, self.beta1, self.beta2, self.beta3]\n",
    "        \n",
    "       \n",
    "    def forward(self, X, training):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        training: bool. True if it's training and False if it's predicting\n",
    "        \"\"\"\n",
    "        def compute_output(X, training):\n",
    "            BN_eps = 1e-5\n",
    "            # Cast X to float32\n",
    "            X_tf = tf.cast(X, dtype=tf.float32)\n",
    "            \n",
    "#             #set the dropout prob\n",
    "#             prob = self.drop_prob\n",
    "            \n",
    "            # BN in input layer\n",
    "            if training:\n",
    "                # compute the sample_mean and sample_var for current batch\n",
    "                sample_mean0 = tf.math.reduce_mean(X_tf, axis=0)\n",
    "                sample_var0 = tf.math.reduce_variance(X_tf, axis=0)\n",
    "                X_tf = self.batch_norm(X_tf, self.gamma0, self.beta0, sample_mean0, sample_var0, BN_eps)\n",
    "                \n",
    "                # update the self.moving_mean0 and self.moving_var0\n",
    "                self.assign_moving_average(self.moving_mean0, sample_mean0)\n",
    "                self.assign_moving_average(self.moving_var0, sample_var0)\n",
    "            else:\n",
    "                # using the updated moving_mean and moving_var for batch normalization\n",
    "                X_tf = self.batch_norm(X_tf, self.gamma0, self.beta0, self.moving_mean0, self.moving_var0, BN_eps)\n",
    "\n",
    "            # Remember to normalize your dataset before moving forward\n",
    "            # Compute values in hidden layer 1\n",
    "            what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "\n",
    "            # activation in hiddent layer 1\n",
    "            hhat1 = tf.nn.relu(what1)\n",
    "            \n",
    "            # BN in hidden layer 1\n",
    "            if training:\n",
    "                # compute the sample_mean and sample_var for current batch\n",
    "                sample_mean1 = tf.math.reduce_mean(hhat1, axis=0)\n",
    "                sample_var1 = tf.math.reduce_variance(hhat1, axis=0)\n",
    "                hhat1 = self.batch_norm(hhat1, self.gamma1, self.beta1, sample_mean1, sample_var1, BN_eps)\n",
    "                \n",
    "                # update the self.moving_mean1 and self.moving_var1\n",
    "                self.assign_moving_average(self.moving_mean1, sample_mean1)\n",
    "                self.assign_moving_average(self.moving_var1, sample_var1)\n",
    "            else:\n",
    "                # using the updated moving_mean and moving_var for batch normalization\n",
    "                hhat1 = self.batch_norm(hhat1, self.gamma1, self.beta1, self.moving_mean1, self.moving_var1, BN_eps)\n",
    "\n",
    "            # Compute values in hidden layer 2\n",
    "            what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "            \n",
    "            # activation in hiddent layer 2\n",
    "            hhat2 = tf.nn.relu(what2)\n",
    "            \n",
    "            # BN in hidden layer 2\n",
    "            if training:\n",
    "                # compute the sample_mean and sample_var for current batch\n",
    "                sample_mean2 = tf.math.reduce_mean(hhat2, axis=0)\n",
    "                sample_var2 = tf.math.reduce_variance(hhat2, axis=0)\n",
    "                hhat2 = self.batch_norm(hhat2, self.gamma2, self.beta2, sample_mean2, sample_var2, BN_eps)\n",
    "                \n",
    "                # update the self.moving_mean2 and self.moving_var2\n",
    "                self.assign_moving_average(self.moving_mean2, sample_mean2)\n",
    "                self.assign_moving_average(self.moving_var2, sample_var2)\n",
    "            else:\n",
    "                # using the updated moving_mean and moving_var for batch normalization\n",
    "                hhat2 = self.batch_norm(hhat2, self.gamma2, self.beta2, self.moving_mean2, self.moving_var2, BN_eps)\n",
    "            \n",
    "            # Compute values in hidden layer 3\n",
    "            what3 = tf.matmul(hhat2, self.W3) + self.b3\n",
    "            \n",
    "            # activation in hiddent layer 3\n",
    "            hhat3 = tf.nn.relu(what3)\n",
    "            \n",
    "            # BN in hidden layer 3\n",
    "            if training:\n",
    "                # compute the sample_mean and sample_var for current batch\n",
    "                sample_mean3 = tf.math.reduce_mean(hhat3, axis=0)\n",
    "                sample_var3 = tf.math.reduce_variance(hhat3, axis=0)\n",
    "                hhat3 = self.batch_norm(hhat3, self.gamma3, self.beta3, sample_mean3, sample_var3, BN_eps)\n",
    "                \n",
    "                # update the self.moving_mean3 and self.moving_var3\n",
    "                self.assign_moving_average(self.moving_mean3, sample_mean3)\n",
    "                self.assign_moving_average(self.moving_var3, sample_var3)\n",
    "            else:\n",
    "                # using the updated moving_mean and moving_var for batch normalization\n",
    "                hhat3 = self.batch_norm(hhat3, self.gamma3, self.beta3, self.moving_mean3, self.moving_var3, BN_eps)\n",
    "            \n",
    "            # Compute output\n",
    "            what4 = tf.matmul(hhat3, self.W4) + self.b4\n",
    "            output = tf.nn.softmax(what4)\n",
    "\n",
    "            return output\n",
    "        \n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = compute_output(X, training)\n",
    "        else:\n",
    "            self.y = compute_output(X, training)\n",
    "\n",
    "        return self.y\n",
    "    \n",
    "    def assign_moving_average(self, variable, value):\n",
    "        momentum = 0.99\n",
    "        delta = variable * momentum + value * (1 - momentum)\n",
    "        return variable.assign(delta)\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits = False)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "    \n",
    "    def batch_norm(self, X, gamma, beta, moving_mean, moving_var, eps):\n",
    "        # Compute reciprocal of square root of the moving variance elementwise\n",
    "        inv = tf.cast(tf.math.rsqrt(moving_var + eps), X.dtype)\n",
    "        # Scale and shift\n",
    "        inv *= gamma\n",
    "        Y = X * inv + (beta - moving_mean * inv)\n",
    "        return Y\n",
    "\n",
    "    def backward(self, X_train, y_train, hyperparams, method='sgd'):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train, training = True)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "    \n",
    "\n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))\n",
    "    \n",
    "#     def dropout_layer(self,X, dropout):\n",
    "#         assert 0 <= dropout <= 1\n",
    "#         # In this case, all elements are dropped out\n",
    "#         if dropout == 1:\n",
    "#             return tf.zeros_like(X)\n",
    "#         # In this case, all elements are kept\n",
    "#         if dropout == 0:\n",
    "#             return X\n",
    "#         mask = tf.random.uniform(\n",
    "#             shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\n",
    "#         return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with SGD from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.3386 - Val loss: 0.3887 - Test loss: 0.4193 - Train acc:= 87.95% - Val acc:= 86.08% - Test acc:= 85.43%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.2734 - Val loss: 0.3663 - Test loss: 0.3969 - Train acc:= 90.28% - Val acc:= 86.77% - Test acc:= 86.46%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.2307 - Val loss: 0.3720 - Test loss: 0.4037 - Train acc:= 91.85% - Val acc:= 87.17% - Test acc:= 86.77%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.1971 - Val loss: 0.3943 - Test loss: 0.4268 - Train acc:= 93.22% - Val acc:= 87.04% - Test acc:= 86.70%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.1701 - Val loss: 0.4266 - Test loss: 0.4619 - Train acc:= 94.20% - Val acc:= 86.84% - Test acc:= 86.64%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.1498 - Val loss: 0.4670 - Test loss: 0.5097 - Train acc:= 94.98% - Val acc:= 86.73% - Test acc:= 86.49%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.1350 - Val loss: 0.5126 - Test loss: 0.5631 - Train acc:= 95.54% - Val acc:= 86.62% - Test acc:= 86.50%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.1259 - Val loss: 0.5599 - Test loss: 0.6179 - Train acc:= 95.93% - Val acc:= 86.51% - Test acc:= 86.19%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.1214 - Val loss: 0.6029 - Test loss: 0.6673 - Train acc:= 96.24% - Val acc:= 86.60% - Test acc:= 86.35%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.1190 - Val loss: 0.6416 - Test loss: 0.7095 - Train acc:= 96.42% - Val acc:= 86.49% - Test acc:= 86.45%\n",
      "\n",
      "Total time taken (in seconds): 2201.92\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 0.3257 - Val loss: 0.3821 - Test loss: 0.4105 - Train acc:= 88.35% - Val acc:= 86.18% - Test acc:= 85.52%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 0.2636 - Val loss: 0.3585 - Test loss: 0.3915 - Train acc:= 90.58% - Val acc:= 86.90% - Test acc:= 86.49%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 0.2206 - Val loss: 0.3593 - Test loss: 0.3950 - Train acc:= 92.12% - Val acc:= 87.20% - Test acc:= 86.63%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 0.1865 - Val loss: 0.3760 - Test loss: 0.4124 - Train acc:= 93.52% - Val acc:= 87.29% - Test acc:= 86.69%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 0.1600 - Val loss: 0.4047 - Test loss: 0.4409 - Train acc:= 94.47% - Val acc:= 87.15% - Test acc:= 86.44%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 0.1350 - Val loss: 0.4418 - Test loss: 0.4770 - Train acc:= 95.53% - Val acc:= 87.02% - Test acc:= 86.46%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 0.1121 - Val loss: 0.4782 - Test loss: 0.5133 - Train acc:= 96.46% - Val acc:= 86.85% - Test acc:= 86.50%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 0.0986 - Val loss: 0.5202 - Test loss: 0.5527 - Train acc:= 96.92% - Val acc:= 86.75% - Test acc:= 86.42%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 0.0841 - Val loss: 0.5578 - Test loss: 0.5905 - Train acc:= 97.55% - Val acc:= 86.76% - Test acc:= 86.30%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 0.0744 - Val loss: 0.5919 - Test loss: 0.6283 - Train acc:= 97.89% - Val acc:= 86.75% - Test acc:= 86.32%\n",
      "\n",
      "Total time taken (in seconds): 2205.79\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 0.3319 - Val loss: 0.3867 - Test loss: 0.4102 - Train acc:= 88.32% - Val acc:= 85.90% - Test acc:= 85.29%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 0.2662 - Val loss: 0.3634 - Test loss: 0.3847 - Train acc:= 90.71% - Val acc:= 87.25% - Test acc:= 86.55%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 0.2228 - Val loss: 0.3641 - Test loss: 0.3834 - Train acc:= 92.33% - Val acc:= 87.52% - Test acc:= 86.97%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 0.1865 - Val loss: 0.3764 - Test loss: 0.3962 - Train acc:= 93.67% - Val acc:= 87.48% - Test acc:= 87.01%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 0.1553 - Val loss: 0.4002 - Test loss: 0.4218 - Train acc:= 94.86% - Val acc:= 87.42% - Test acc:= 86.95%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 0.1282 - Val loss: 0.4288 - Test loss: 0.4523 - Train acc:= 95.92% - Val acc:= 87.34% - Test acc:= 86.87%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 0.1096 - Val loss: 0.4676 - Test loss: 0.4953 - Train acc:= 96.59% - Val acc:= 87.19% - Test acc:= 86.56%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 0.0992 - Val loss: 0.5094 - Test loss: 0.5429 - Train acc:= 96.97% - Val acc:= 86.87% - Test acc:= 86.51%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 0.0808 - Val loss: 0.5417 - Test loss: 0.5763 - Train acc:= 97.72% - Val acc:= 86.84% - Test acc:= 86.66%\n",
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 0.0742 - Val loss: 0.5781 - Test loss: 0.6172 - Train acc:= 97.95% - Val acc:= 86.99% - Test acc:= 86.51%\n",
      "\n",
      "Total time taken (in seconds): 2161.65\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                regularizer=None, R_lambda = 1e-4, drop_prob=0.)\n",
    "#     mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':1e-4}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs, training = True)\n",
    "\n",
    "            #use SGD to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams,'sgd')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train, training = False)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val, training = False)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test, training = False)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
