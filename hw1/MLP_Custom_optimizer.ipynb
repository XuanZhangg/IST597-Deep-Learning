{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: (50000, 784)\n",
      "val_size: (10000, 784)\n",
      "test_size: (10000, 784)\n",
      "train_output_size (50000,)\n",
      "val_output_size: (10000,)\n",
      "test_output_size (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load and normalize data\n",
    "(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "#(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = tf.reshape(X_train, (X_train.shape[0],-1))/255\n",
    "X_test = tf.reshape(X_test, (X_test.shape[0],-1))/255\n",
    "\n",
    "#reserve the last 10000 training examples for validation\n",
    "X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "print(\"train_size:\", X_train.shape)\n",
    "print(\"val_size:\", X_val.shape)\n",
    "print(\"test_size:\", X_test.shape)\n",
    "print(\"train_output_size\", y_train.shape)\n",
    "print(\"val_output_size:\", y_val.shape)\n",
    "print(\"test_output_size\", y_test.shape)\n",
    "\n",
    "size_input = X_train.shape[1]\n",
    "size_output = len(set(y_train))\n",
    "size_hidden1 = 256\n",
    "size_hidden2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        regularizer: str or None\n",
    "        R_lambda: the parameter for regularizer\n",
    "        drop_prob: 0 to 1\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_output, device\n",
    "        \n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "        \n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
    "        # Initialize biases for hidden layer 1\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))# 0 or constant(0.01)\n",
    "\n",
    "        # Initialize weights between hidden layer 1 and hidden layer 2\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "        # Initialize biases for hidden layer 2\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "\n",
    "         # Initialize weights between hidden layer 2 and output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "        # Initialize the state of custom optimizer\n",
    "        #note that this is for custom optimizer but not for the variables, therefore they are just zeros. \n",
    "        self.v_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.v_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.u_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.u_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.m_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.m_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        \n",
    "        self.v_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.v_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.u_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.u_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.v_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.v_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.m_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.m_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        \n",
    "        self.v_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.v_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.u_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.u_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.m_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.m_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        \n",
    "        self.v_state = [self.v_W1,self.v_W2,self.v_W3,self.v_b1,self.v_b2,self.v_b3]\n",
    "        self.u_state = [self.u_W1,self.u_W2,self.u_W3,self.u_b1,self.u_b2,self.u_b3]\n",
    "        self.m_state = [self.m_W1,self.m_W2,self.m_W3,self.m_b1,self.m_b2,self.m_b3]\n",
    "       \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        def compute_output(X):\n",
    "            # Cast X to float32\n",
    "            X_tf = tf.cast(X, dtype=tf.float32)\n",
    "            \n",
    "            #set the dropout prob\n",
    "            prob = self.drop_prob\n",
    "\n",
    "            # Remember to normalize your dataset before moving forward\n",
    "            # Compute values in hidden layer 1\n",
    "            what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "            hhat1 = tf.nn.experimental.stateless_dropout(tf.nn.relu(what1), rate = prob, seed = [1,0])/(1-prob)\n",
    "\n",
    "            # Compute values in hidden layer 2\n",
    "            what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "            hhat2 = tf.nn.experimental.stateless_dropout(tf.nn.relu(what2), rate = prob, seed = [1,0])/(1-prob)\n",
    "#             print('layer-relu')\n",
    "#             print(hhat2[:10])\n",
    "\n",
    "            # Compute output\n",
    "            output = tf.nn.softmax(tf.matmul(hhat2, self.W3) + self.b3)\n",
    "#             print('output')\n",
    "#             print(output[:10])\n",
    "\n",
    "            return output\n",
    "        \n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = compute_output(X)\n",
    "        else:\n",
    "            self.y = compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true,y_pred, from_logits = False)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "        \n",
    "    def backward(self, X_train, y_train, hyperparams, method='custom'):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        elif method == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-6,amsgrad=False,name='Adam')\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        elif method == 'custom':\n",
    "            #Custom optimizer\n",
    "            beta1,beta2,beta3,eps = 0.9,0.999,0.999987,1e-8\n",
    "\n",
    "            for p,m,v,u,grad in zip(self.variables, self.m_state, self.v_state, self.u_state, grads):\n",
    "                m[:].assign(beta1 * m  + (1 - beta1) * grad)\n",
    "                v[:].assign(beta2 * v  + (1 - beta2) * tf.math.square(grad))\n",
    "                u[:].assign(beta3 * u  + (1 - beta3) * tf.math.pow(grad, 3))\n",
    "                m_bias_corr = m / (1 - beta1 ** hyperparams['t'])\n",
    "                v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "                u_bias_corr = u / (1 - beta2 ** hyperparams['t'])\n",
    "                p[:].assign(p - hyperparams['lr'] * m_bias_corr/ (tf.math.sqrt(v_bias_corr) + eps*tf.math.sign(u_bias_corr)*tf.math.pow(abs(u_bias_corr),1.0/3.0) + eps))\n",
    "\n",
    "\n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))\n",
    "    \n",
    "#     def dropout_layer(self,X, dropout):\n",
    "#         assert 0 <= dropout <= 1\n",
    "#         # In this case, all elements are dropped out\n",
    "#         if dropout == 1:\n",
    "#             return tf.zeros_like(X)\n",
    "#         # In this case, all elements are kept\n",
    "#         if dropout == 0:\n",
    "#             return X\n",
    "#         mask = tf.random.uniform(\n",
    "#             shape=tf.shape(X), minval=0, maxval=1) < 1 - dropout\n",
    "#         return tf.cast(mask, dtype=tf.float32) * X / (1.0 - dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with Custom Opitmizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.6613 - Val loss: 0.6259 - Test loss: 0.6429 - Train acc:= 86.51% - Val acc:= 88.28% - Test acc:= 87.40%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.4177 - Val loss: 0.3879 - Test loss: 0.4019 - Train acc:= 89.74% - Val acc:= 90.55% - Test acc:= 90.25%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.3387 - Val loss: 0.3142 - Test loss: 0.3264 - Train acc:= 90.83% - Val acc:= 91.52% - Test acc:= 91.29%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.3001 - Val loss: 0.2795 - Test loss: 0.2909 - Train acc:= 91.63% - Val acc:= 92.32% - Test acc:= 91.83%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.2752 - Val loss: 0.2577 - Test loss: 0.2686 - Train acc:= 92.19% - Val acc:= 92.71% - Test acc:= 92.38%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.2566 - Val loss: 0.2417 - Test loss: 0.2522 - Train acc:= 92.62% - Val acc:= 93.11% - Test acc:= 92.85%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.2413 - Val loss: 0.2286 - Test loss: 0.2387 - Train acc:= 93.04% - Val acc:= 93.38% - Test acc:= 93.27%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.2280 - Val loss: 0.2174 - Test loss: 0.2271 - Train acc:= 93.45% - Val acc:= 93.77% - Test acc:= 93.49%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.2161 - Val loss: 0.2075 - Test loss: 0.2167 - Train acc:= 93.79% - Val acc:= 94.07% - Test acc:= 93.72%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.2053 - Val loss: 0.1984 - Test loss: 0.2071 - Train acc:= 94.08% - Val acc:= 94.34% - Test acc:= 93.95%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.1954 - Val loss: 0.1901 - Test loss: 0.1984 - Train acc:= 94.33% - Val acc:= 94.63% - Test acc:= 94.17%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.1862 - Val loss: 0.1825 - Test loss: 0.1902 - Train acc:= 94.63% - Val acc:= 94.94% - Test acc:= 94.40%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.1776 - Val loss: 0.1755 - Test loss: 0.1826 - Train acc:= 94.94% - Val acc:= 95.22% - Test acc:= 94.60%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.1696 - Val loss: 0.1690 - Test loss: 0.1756 - Train acc:= 95.16% - Val acc:= 95.42% - Test acc:= 94.88%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.1622 - Val loss: 0.1630 - Test loss: 0.1690 - Train acc:= 95.36% - Val acc:= 95.51% - Test acc:= 95.11%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.1552 - Val loss: 0.1575 - Test loss: 0.1629 - Train acc:= 95.60% - Val acc:= 95.64% - Test acc:= 95.26%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.1486 - Val loss: 0.1523 - Test loss: 0.1573 - Train acc:= 95.82% - Val acc:= 95.75% - Test acc:= 95.40%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.1425 - Val loss: 0.1475 - Test loss: 0.1519 - Train acc:= 95.99% - Val acc:= 95.92% - Test acc:= 95.46%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.1367 - Val loss: 0.1430 - Test loss: 0.1470 - Train acc:= 96.17% - Val acc:= 96.09% - Test acc:= 95.61%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.1313 - Val loss: 0.1388 - Test loss: 0.1424 - Train acc:= 96.33% - Val acc:= 96.16% - Test acc:= 95.73%\n",
      "\n",
      "Total time taken (in seconds): 1979.21\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 1\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "    #mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams,'custom')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with SGD from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 2.1351 - Val loss: 2.1312 - Test loss: 2.1306 - Train acc:= 48.99% - Val acc:= 50.64% - Test acc:= 51.00%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 1.8067 - Val loss: 1.7946 - Test loss: 1.7936 - Train acc:= 62.99% - Val acc:= 65.26% - Test acc:= 64.62%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 1.3465 - Val loss: 1.3223 - Test loss: 1.3248 - Train acc:= 72.21% - Val acc:= 74.83% - Test acc:= 73.47%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 1.0145 - Val loss: 0.9811 - Test loss: 0.9903 - Train acc:= 77.97% - Val acc:= 80.61% - Test acc:= 78.92%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.8168 - Val loss: 0.7790 - Test loss: 0.7928 - Train acc:= 81.43% - Val acc:= 83.52% - Test acc:= 82.39%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.6924 - Val loss: 0.6532 - Test loss: 0.6689 - Train acc:= 83.64% - Val acc:= 85.77% - Test acc:= 84.67%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.6091 - Val loss: 0.5702 - Test loss: 0.5862 - Train acc:= 85.10% - Val acc:= 86.94% - Test acc:= 86.07%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.5507 - Val loss: 0.5130 - Test loss: 0.5283 - Train acc:= 86.13% - Val acc:= 87.87% - Test acc:= 86.96%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.5080 - Val loss: 0.4717 - Test loss: 0.4860 - Train acc:= 86.94% - Val acc:= 88.39% - Test acc:= 87.78%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.4755 - Val loss: 0.4408 - Test loss: 0.4541 - Train acc:= 87.56% - Val acc:= 88.88% - Test acc:= 88.35%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.4501 - Val loss: 0.4169 - Test loss: 0.4293 - Train acc:= 88.07% - Val acc:= 89.23% - Test acc:= 88.85%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.4298 - Val loss: 0.3980 - Test loss: 0.4095 - Train acc:= 88.47% - Val acc:= 89.51% - Test acc:= 89.13%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.4131 - Val loss: 0.3825 - Test loss: 0.3933 - Train acc:= 88.80% - Val acc:= 89.74% - Test acc:= 89.38%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.3991 - Val loss: 0.3696 - Test loss: 0.3799 - Train acc:= 89.07% - Val acc:= 89.96% - Test acc:= 89.63%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.3872 - Val loss: 0.3587 - Test loss: 0.3686 - Train acc:= 89.33% - Val acc:= 90.19% - Test acc:= 89.81%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.3769 - Val loss: 0.3493 - Test loss: 0.3588 - Train acc:= 89.55% - Val acc:= 90.40% - Test acc:= 90.04%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.3678 - Val loss: 0.3411 - Test loss: 0.3503 - Train acc:= 89.76% - Val acc:= 90.62% - Test acc:= 90.14%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.3598 - Val loss: 0.3338 - Test loss: 0.3428 - Train acc:= 89.93% - Val acc:= 90.75% - Test acc:= 90.33%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.3525 - Val loss: 0.3272 - Test loss: 0.3361 - Train acc:= 90.09% - Val acc:= 90.85% - Test acc:= 90.53%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.3460 - Val loss: 0.3212 - Test loss: 0.3300 - Train acc:= 90.27% - Val acc:= 90.95% - Test acc:= 90.66%\n",
      "\n",
      "Total time taken (in seconds): 799.70\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 1\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "    #mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams,'sgd')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model with ADAM from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.8452 - Val loss: 0.8099 - Test loss: 0.8261 - Train acc:= 83.20% - Val acc:= 85.36% - Test acc:= 84.48%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.4816 - Val loss: 0.4466 - Test loss: 0.4625 - Train acc:= 88.40% - Val acc:= 89.68% - Test acc:= 89.27%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.3783 - Val loss: 0.3482 - Test loss: 0.3610 - Train acc:= 89.85% - Val acc:= 90.71% - Test acc:= 90.53%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.3338 - Val loss: 0.3070 - Test loss: 0.3183 - Train acc:= 90.61% - Val acc:= 91.37% - Test acc:= 91.14%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.3083 - Val loss: 0.2838 - Test loss: 0.2943 - Train acc:= 91.09% - Val acc:= 91.92% - Test acc:= 91.54%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.2909 - Val loss: 0.2681 - Test loss: 0.2783 - Train acc:= 91.50% - Val acc:= 92.19% - Test acc:= 91.93%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.2775 - Val loss: 0.2563 - Test loss: 0.2662 - Train acc:= 91.82% - Val acc:= 92.40% - Test acc:= 92.30%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.2665 - Val loss: 0.2466 - Test loss: 0.2563 - Train acc:= 92.13% - Val acc:= 92.63% - Test acc:= 92.47%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.2569 - Val loss: 0.2382 - Test loss: 0.2478 - Train acc:= 92.40% - Val acc:= 92.99% - Test acc:= 92.72%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.2483 - Val loss: 0.2306 - Test loss: 0.2401 - Train acc:= 92.63% - Val acc:= 93.08% - Test acc:= 92.89%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.2403 - Val loss: 0.2237 - Test loss: 0.2330 - Train acc:= 92.85% - Val acc:= 93.19% - Test acc:= 93.08%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.2329 - Val loss: 0.2171 - Test loss: 0.2263 - Train acc:= 93.07% - Val acc:= 93.36% - Test acc:= 93.25%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.2260 - Val loss: 0.2110 - Test loss: 0.2200 - Train acc:= 93.31% - Val acc:= 93.66% - Test acc:= 93.48%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.2194 - Val loss: 0.2052 - Test loss: 0.2141 - Train acc:= 93.50% - Val acc:= 93.87% - Test acc:= 93.66%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.2131 - Val loss: 0.1997 - Test loss: 0.2084 - Train acc:= 93.70% - Val acc:= 94.06% - Test acc:= 93.80%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.2072 - Val loss: 0.1946 - Test loss: 0.2029 - Train acc:= 93.87% - Val acc:= 94.19% - Test acc:= 93.87%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.2016 - Val loss: 0.1896 - Test loss: 0.1978 - Train acc:= 94.05% - Val acc:= 94.43% - Test acc:= 94.12%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.1962 - Val loss: 0.1849 - Test loss: 0.1928 - Train acc:= 94.19% - Val acc:= 94.53% - Test acc:= 94.24%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.1911 - Val loss: 0.1805 - Test loss: 0.1881 - Train acc:= 94.36% - Val acc:= 94.74% - Test acc:= 94.40%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.1862 - Val loss: 0.1763 - Test loss: 0.1835 - Train acc:= 94.51% - Val acc:= 94.93% - Test acc:= 94.50%\n",
      "\n",
      "Total time taken (in seconds): 1439.69\n"
     ]
    }
   ],
   "source": [
    "#save the model for tuning\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    # Overwrites any existing file.\n",
    "    with open(filename, 'wb') as file:  \n",
    "        pickle.dump(obj, file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):\n",
    "    # Open the file in binary mode\n",
    "    with open(filename, 'rb') as file:  \n",
    "        # Call load method to deserialze\n",
    "        return pickle.load(file)\n",
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 1\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "#set the train_record\n",
    "train_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "train_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "val_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "val_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "test_loss_record = [[] for _ in range(NUM_SIM)]\n",
    "test_acc_record = [[] for _ in range(NUM_SIM)]\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_DIY = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "    #mlp_DIY = load_object('mlp_DIY.pkl')\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_DIY.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_DIY.backward(inputs, outputs, hyperparams,'adam')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_DIY.forward(X_train)\n",
    "            train_loss = np.sum(mlp_DIY.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_DIY.accuracy(logits,y_train)/len(y_train)\n",
    "            train_loss_record[num_sim].append(train_loss)\n",
    "            train_acc_record[num_sim].append(train_acc)\n",
    "\n",
    "            logits = mlp_DIY.forward(X_val)\n",
    "            val_loss = np.sum(mlp_DIY.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_DIY.accuracy(logits,y_val)/len(y_val)\n",
    "            val_loss_record[num_sim].append(val_loss)\n",
    "            val_acc_record[num_sim].append(val_acc)\n",
    "            \n",
    "            logits = mlp_DIY.forward(X_test)\n",
    "            test_loss = np.sum(mlp_DIY.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_DIY.accuracy(logits,y_test)/len(y_test)\n",
    "            test_loss_record[num_sim].append(test_loss)\n",
    "            test_acc_record[num_sim].append(test_acc)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
